{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782a7ba4",
   "metadata": {},
   "source": [
    "# AutoRAG-style RAG Pipeline\n",
    "\n",
    "Этот ноутбук — автоконфигурируемый пайплайн в стиле AutoRAG: chunking → индекс → hybrid retrieval → cross-encoder rerank → LLM (Qwen) → submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66439170",
   "metadata": {},
   "source": [
    "## Block 0. Setup & конфиг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08abfdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install qdrant_client rank-bm25 sentence-transformers datasets nltk langchain --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7822e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from qdrant_client import QdrantClient, models\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "DATA_DIR = Path(\"/kaggle/input\")\n",
    "DOC_SOURCE = \"hf\"  # \"hf\" или \"local\"\n",
    "\n",
    "HF_DATASET_NAME = \"your/dataset\"\n",
    "HF_SPLIT_DOCS = \"train\"\n",
    "HF_SPLIT_QA = \"validation\"\n",
    "\n",
    "RAW_DOCS_DIR = DATA_DIR / \"docs\"\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 250\n",
    "MIN_CHARS = 50\n",
    "\n",
    "TOP_K_RETRIEVAL = 10\n",
    "TOP_K_FINAL = 5\n",
    "\n",
    "EMBED_MODEL_ID = \"intfloat/multilingual-e5-large\"\n",
    "CROSS_ENCODER_ID = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "GEN_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/1.5b-instruct/1\"\n",
    "MAX_NEW_TOKENS = 256\n",
    "TEMP = 0.2\n",
    "TOP_P = 0.9\n",
    "\n",
    "QDRANT_COLLECTION = \"autorag_like_rag\"\n",
    "EMBED_DIM = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186d7e13",
   "metadata": {},
   "source": [
    "## Block 1. Загрузка данных (corpus + QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3503bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs_and_qa() -> Tuple[List[Dict[str, Any]], pd.DataFrame]:\n",
    "    docs = []\n",
    "    qa_df = None\n",
    "\n",
    "    if DOC_SOURCE == \"hf\":\n",
    "        dataset_docs = load_dataset(HF_DATASET_NAME, split=HF_SPLIT_DOCS)\n",
    "        for i, row in enumerate(dataset_docs):\n",
    "            text = str(row.get(\"content\", \"\"))\n",
    "            if text.strip():\n",
    "                docs.append({\"doc_id\": i, \"text\": text, \"meta\": {k: row[k] for k in row if k != \"content\"}})\n",
    "\n",
    "        dataset_qa = load_dataset(HF_DATASET_NAME, split=HF_SPLIT_QA)\n",
    "        qa_df = dataset_qa.to_pandas()\n",
    "    else:\n",
    "        for i, p in enumerate(sorted(RAW_DOCS_DIR.glob(\"*.txt\"))):\n",
    "            text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            docs.append({\"doc_id\": i, \"text\": text, \"meta\": {\"filename\": p.name}})\n",
    "        qa_df = pd.read_csv(DATA_DIR / \"qa.csv\")\n",
    "\n",
    "    print(f\"Loaded {len(docs)} docs, QA shape = {None if qa_df is None else qa_df.shape}\")\n",
    "    return docs, qa_df\n",
    "\n",
    "\n",
    "docs, qa_df = load_docs_and_qa()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbfb9ef",
   "metadata": {},
   "source": [
    "## Block 2. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recursive_chunks(\n",
    "    docs: List[Dict[str, Any]],\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "    chunk_overlap: int = CHUNK_OVERLAP,\n",
    "    min_chars: int = MIN_CHARS\n",
    ") -> List[Dict[str, Any]]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "    all_chunks = []\n",
    "    for d in tqdm(docs, desc=\"Chunking docs (recursive)\"):\n",
    "        doc_id = d[\"doc_id\"]\n",
    "        text = d[\"text\"]\n",
    "        chunks_loc = splitter.split_text(text)\n",
    "        for idx, ch in enumerate(chunks_loc):\n",
    "            if len(ch.strip()) < min_chars:\n",
    "                continue\n",
    "            all_chunks.append(\n",
    "                {\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": f\"{doc_id}_{idx}\",\n",
    "                    \"text\": ch,\n",
    "                    \"meta\": d.get(\"meta\", {}),\n",
    "                }\n",
    "            )\n",
    "    print(\"Total chunks:\", len(all_chunks))\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "chunks = make_recursive_chunks(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b28672",
   "metadata": {},
   "source": [
    "## Block 3. Эмбеддинги + Qdrant + BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b5be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\n",
    "    EMBED_MODEL_ID,\n",
    "    device=str(device),\n",
    ")\n",
    "embedding_model.eval()\n",
    "\n",
    "test_vec = embedding_model.encode([\"test\"], normalize_embeddings=True)\n",
    "EMBED_DIM = test_vec.shape[1]\n",
    "print(\"Embedding dim:\", EMBED_DIM)\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=QDRANT_COLLECTION,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=EMBED_DIM,\n",
    "        distance=models.Distance.COSINE,\n",
    "    ),\n",
    "    on_disk_payload=False,\n",
    ")\n",
    "print(\"Qdrant collection created:\", QDRANT_COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496cd03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_chunks_qdrant(chunks: List[Dict[str, Any]]) -> None:\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    vectors = embedding_model.encode(\n",
    "        texts,\n",
    "        batch_size=64,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True,\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    payloads = []\n",
    "    for i, ch in enumerate(chunks):\n",
    "        payloads.append(\n",
    "            {\n",
    "                \"doc_id\": ch[\"doc_id\"],\n",
    "                \"chunk_id\": ch[\"chunk_id\"],\n",
    "                **(ch.get(\"meta\") or {}),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    client.upsert(\n",
    "        collection_name=QDRANT_COLLECTION,\n",
    "        points=[\n",
    "            models.PointStruct(id=int(i), vector=vectors[i].tolist(), payload=payloads[i])\n",
    "            for i in range(len(vectors))\n",
    "        ],\n",
    "    )\n",
    "    print(\"Indexed to Qdrant:\", len(vectors))\n",
    "\n",
    "\n",
    "index_chunks_qdrant(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_chunks = [word_tokenize(c[\"text\"]) for c in chunks]\n",
    "bm25 = BM25Okapi(tokenized_chunks)\n",
    "print(\"BM25 corpus ready:\", len(tokenized_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a608c",
   "metadata": {},
   "source": [
    "## Block 4. Ретраиверы (semantic / BM25 / hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ffc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, top_k: int = TOP_K_RETRIEVAL):\n",
    "    query_vec = embedding_model.encode([query], normalize_embeddings=True).astype(np.float32)[0].tolist()\n",
    "    hits = client.search(\n",
    "        collection_name=QDRANT_COLLECTION,\n",
    "        query_vector=query_vec,\n",
    "        limit=top_k,\n",
    "    )\n",
    "    results = []\n",
    "    for h in hits:\n",
    "        results.append(\n",
    "            {\n",
    "                \"score_semantic\": h.score,\n",
    "                \"chunk_id\": h.payload.get(\"chunk_id\"),\n",
    "                \"doc_id\": h.payload.get(\"doc_id\"),\n",
    "            }\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "def bm25_search(query: str, top_k: int = TOP_K_RETRIEVAL):\n",
    "    tokenized_query = word_tokenize(query)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    idx_scores = list(enumerate(scores))\n",
    "    idx_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    results = []\n",
    "    for idx, sc in idx_scores[:top_k]:\n",
    "        ch = chunks[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"score_bm25\": float(sc),\n",
    "                \"chunk_id\": ch[\"chunk_id\"],\n",
    "                \"doc_id\": ch[\"doc_id\"],\n",
    "            }\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "def hybrid_search(query: str, alpha: float = 0.5, top_k: int = TOP_K_RETRIEVAL):\n",
    "    sem = semantic_search(query, top_k=top_k * 2)\n",
    "    bm = bm25_search(query, top_k=top_k * 2)\n",
    "\n",
    "    sem_map = {(r[\"doc_id\"], r[\"chunk_id\"]): r for r in sem}\n",
    "    bm_map = {(r[\"doc_id\"], r[\"chunk_id\"]): r for r in bm}\n",
    "\n",
    "    all_keys = set(sem_map.keys()) | set(bm_map.keys())\n",
    "    results = []\n",
    "\n",
    "    max_sem = max([r[\"score_semantic\"] for r in sem], default=1.0)\n",
    "    max_bm = max([r[\"score_bm25\"] for r in bm], default=1.0)\n",
    "\n",
    "    for key in all_keys:\n",
    "        sem_s = sem_map.get(key, {}).get(\"score_semantic\", 0.0) / max_sem\n",
    "        bm_s = bm_map.get(key, {}).get(\"score_bm25\", 0.0) / max_bm\n",
    "        score = alpha * sem_s + (1 - alpha) * bm_s\n",
    "        doc_id, chunk_id = key\n",
    "        results.append(\n",
    "            {\n",
    "                \"score_hybrid\": float(score),\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    results.sort(key=lambda x: x[\"score_hybrid\"], reverse=True)\n",
    "    return results[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8ee66",
   "metadata": {},
   "source": [
    "## Block 5. Cross-encoder reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88006f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder(CROSS_ENCODER_ID, device=str(device))\n",
    "\n",
    "\n",
    "def get_chunks_by_ids(chunk_keys: List[Tuple[int, str]]) -> List[str]:\n",
    "    index_map = {(c[\"doc_id\"], c[\"chunk_id\"]): c[\"text\"] for c in chunks}\n",
    "    return [index_map[k] for k in chunk_keys]\n",
    "\n",
    "\n",
    "def rerank_with_cross_encoder(query: str, candidates: List[Dict[str, Any]], top_k: int = TOP_K_FINAL):\n",
    "    if not candidates:\n",
    "        return []\n",
    "\n",
    "    keys = []\n",
    "    for c in candidates:\n",
    "        keys.append((c[\"doc_id\"], c[\"chunk_id\"]))\n",
    "    texts = get_chunks_by_ids(keys)\n",
    "\n",
    "    pair_inputs = []\n",
    "    for t in texts:\n",
    "        pair_inputs.append((query, t))\n",
    "\n",
    "    scores = cross_encoder.predict(pair_inputs)\n",
    "    scored = []\n",
    "    for (doc_id, chunk_id), sc in zip(keys, scores):\n",
    "        scored.append(\n",
    "            {\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"score_ce\": float(sc),\n",
    "            }\n",
    "        )\n",
    "    scored.sort(key=lambda x: x[\"score_ce\"], reverse=True)\n",
    "    return scored[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c4939",
   "metadata": {},
   "source": [
    "## Block 6. LLM-генерация (Qwen) + RAG-answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60752671",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=GEN_MODEL_PATH,\n",
    "    device=0 if device.type == \"cuda\" else -1,\n",
    "    torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant that answers questions using the provided context.\n",
    "Answer in the same language as the question.\n",
    "If the answer cannot be found in the context, say that you don't know and avoid hallucinations.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_prompt(query: str, context_chunks: List[str]) -> str:\n",
    "    context_text = \"\\n\\n\".join(context_chunks)\n",
    "    prompt = f\"{SYSTEM_PROMPT}\\n\\nContext:\\n{context_text}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def llm_answer(query: str, context_chunks: List[str]) -> str:\n",
    "    prompt = build_prompt(query, context_chunks)\n",
    "    out = generation_pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=TEMP,\n",
    "        top_p=TOP_P,\n",
    "        pad_token_id=generation_pipeline.tokenizer.eos_token_id,\n",
    "    )\n",
    "    text = out[0][\"generated_text\"]\n",
    "    if \"Answer:\" in text:\n",
    "        text = text.split(\"Answer:\", 1)[1].strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9990e9",
   "metadata": {},
   "source": [
    "## Block 7. End-to-End RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c354df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer_with_context_and_refs(\n",
    "    query: str,\n",
    "    retrieval_mode: str = \"hybrid\",\n",
    "):\n",
    "    if retrieval_mode == \"semantic\":\n",
    "        initial = semantic_search(query, top_k=TOP_K_RETRIEVAL)\n",
    "    elif retrieval_mode == \"bm25\":\n",
    "        initial = bm25_search(query, top_k=TOP_K_RETRIEVAL)\n",
    "    else:\n",
    "        initial = hybrid_search(query, top_k=TOP_K_RETRIEVAL)\n",
    "\n",
    "    reranked = rerank_with_cross_encoder(query, initial, top_k=TOP_K_FINAL)\n",
    "\n",
    "    index_map = {(c[\"doc_id\"], c[\"chunk_id\"]): c[\"text\"] for c in chunks}\n",
    "    context_chunks = []\n",
    "    refs = []\n",
    "    for r in reranked:\n",
    "        key = (r[\"doc_id\"], r[\"chunk_id\"])\n",
    "        txt = index_map.get(key)\n",
    "        if not txt:\n",
    "            continue\n",
    "        context_chunks.append(txt)\n",
    "        refs.append(\n",
    "            {\n",
    "                \"doc_id\": int(r[\"doc_id\"]),\n",
    "                \"chunk_id\": str(r[\"chunk_id\"]),\n",
    "                \"score_ce\": r[\"score_ce\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    answer = llm_answer(query, context_chunks)\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context_chunks)\n",
    "\n",
    "    refs_dict = {\n",
    "        \"retrieval_mode\": retrieval_mode,\n",
    "        \"top_k\": TOP_K_FINAL,\n",
    "        \"chunks\": refs,\n",
    "    }\n",
    "    return answer, context_text, refs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91c128",
   "metadata": {},
   "source": [
    "## Block 8. Submission (вопросы → ответы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dbcc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_ID_COL = \"id\"\n",
    "QA_QUESTION_COL = \"question\"\n",
    "SUBM_ID_COL = \"id\"\n",
    "SUBM_ANSWER_COL = \"answer\"\n",
    "SUBM_REFS_COL = \"refs_json\"\n",
    "\n",
    "\n",
    "def build_submission(\n",
    "    qa_df: pd.DataFrame,\n",
    "    retrieval_mode: str = \"hybrid\",\n",
    "    output_path: str = \"submission.csv\",\n",
    ") -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, row in tqdm(qa_df.iterrows(), total=len(qa_df), desc=\"Building submission\"):\n",
    "        q_id = row[QA_ID_COL]\n",
    "        q_text = str(row[QA_QUESTION_COL])\n",
    "\n",
    "        try:\n",
    "            answer, context_text, refs_dict = rag_answer_with_context_and_refs(\n",
    "                q_text,\n",
    "                retrieval_mode=retrieval_mode,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            answer = f\"error: {repr(e)}\"\n",
    "            refs_dict = {\"error\": repr(e)}\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                SUBM_ID_COL: q_id,\n",
    "                SUBM_ANSWER_COL: answer,\n",
    "                SUBM_REFS_COL: json.dumps(refs_dict, ensure_ascii=False),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    subm_df = pd.DataFrame(rows)\n",
    "    subm_df.to_csv(output_path, index=False)\n",
    "    print(\"Saved submission to:\", output_path)\n",
    "    return subm_df\n",
    "\n",
    "\n",
    "# раскомментируй, когда qa_df будет готова\n",
    "# submission_df = build_submission(qa_df, retrieval_mode=\"hybrid\", output_path=\"submission.csv\")\n",
    "# submission_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f8655",
   "metadata": {},
   "source": [
    "## Block 9. (Опционально) простая offline-оценка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac323621",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_ANSWER_COL = \"answer\"\n",
    "\n",
    "\n",
    "def simple_exact_match(pred: str, gt: str) -> float:\n",
    "    return float(pred.strip().lower() == gt.strip().lower())\n",
    "\n",
    "\n",
    "def evaluate_on_small_subset(\n",
    "    qa_df: pd.DataFrame,\n",
    "    n: int = 50,\n",
    "    retrieval_mode: str = \"hybrid\",\n",
    "):\n",
    "    sub = qa_df.sample(min(n, len(qa_df)), random_state=SEED)\n",
    "    scores = []\n",
    "    for _, row in tqdm(sub.iterrows(), total=len(sub), desc=\"Eval\"):\n",
    "        q = str(row[QA_QUESTION_COL])\n",
    "        gt = str(row[GT_ANSWER_COL])\n",
    "        pred, _, _ = rag_answer_with_context_and_refs(q, retrieval_mode=retrieval_mode)\n",
    "        scores.append(simple_exact_match(pred, gt))\n",
    "    print(\"Exact match:\", np.mean(scores))\n",
    "\n",
    "\n",
    "# пример вызова:\n",
    "# evaluate_on_small_subset(qa_df, n=30, retrieval_mode=\"hybrid\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}