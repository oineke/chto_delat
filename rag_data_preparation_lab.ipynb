{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a79c713",
   "metadata": {},
   "source": [
    "# RAG Data Preparation Lab\n",
    "\n",
    "Ноутбук-шпаргалка по **подготовке данных для RAG**:\n",
    "\n",
    "- какие бывают форматы входных данных (PDF, TXT, MD, HTML, DOCX, CSV/JSON и т.д.);\n",
    "- как их читать в единый табличный вид (`docs_df` с текстом и метаданными);\n",
    "- стратегии разбиения: по файлам, страницам, разделам;\n",
    "- разные способы чанкинга: по символам, по предложениям, гибридно;\n",
    "- как хранить подготовленный корпус для повторного использования.\n",
    "\n",
    "Фокус: **подготовка корпуса**. Ретривер и LLM здесь специально вынесены за скобки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b492761",
   "metadata": {},
   "source": [
    "## Block 0. Setup & базовый конфиг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0340ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "BASE_DIR = Path('.').resolve()\n",
    "RAW_DIR = BASE_DIR / 'raw_data'   # сюда кладём исходные файлы\n",
    "PROC_DIR = BASE_DIR / 'processed' # сюда складываем подготовленный корпус\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('BASE_DIR:', BASE_DIR)\n",
    "print('RAW_DIR :', RAW_DIR)\n",
    "print('PROC_DIR:', PROC_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673478d8",
   "metadata": {},
   "source": [
    "## Block 1. Какие бывают данные для RAG\n",
    "\n",
    "Типичные источники знаний для RAG-задач:\n",
    "\n",
    "1. **Документы** (длинный текст):\n",
    "   - PDF (книги, отчёты, статьи);\n",
    "   - TXT / MD (plain text, README, документация);\n",
    "   - HTML / web-страницы (зеркало сайта, документации);\n",
    "   - DOCX (отчёты, инструкции).\n",
    "\n",
    "2. **Табличные/структурированные данные**:\n",
    "   - CSV / Parquet (таблицы с фактами, справочники);\n",
    "   - JSON / JSONL (FAQ, QA-пары, конфиги, структурированные документы);\n",
    "   - Базы знаний в виде `id → text`, `id → attributes`.\n",
    "\n",
    "3. **Микс форматов**:\n",
    "   - набор PDF + несколько CSV-таблиц с параметрами;\n",
    "   - docs + QA-файл, где лежат вопросы и ground truth (для обучения/валидации).\n",
    "\n",
    "Идеальная цель подготовки: привести всё к единому формату **таблицы документов**:\n",
    "\n",
    "```text\n",
    "docs_df:\n",
    "  doc_id   (int)\n",
    "  source   (str)   # откуда документ\n",
    "  path     (str)\n",
    "  fmt      (str)   # 'pdf', 'txt', 'md', 'html', 'docx', 'csv_row', ...\n",
    "  title    (str)   # можно вытащить из файла/метаданных\n",
    "  section  (str)   # при необходимости (раздел, глава)\n",
    "  page     (int)   # для pdf/page-based\n",
    "  text     (str)\n",
    "```\n",
    "\n",
    "Дальше все ретриверы/чанкеры работают уже только с `docs_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc1d442",
   "metadata": {},
   "source": [
    "## Block 2. Поиск файлов в `raw_data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b769f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "SUPPORTED_EXTS = {\n",
    "    '.pdf': 'pdf',\n",
    "    '.txt': 'txt',\n",
    "    '.md': 'md',\n",
    "    '.html': 'html',\n",
    "    '.htm': 'html',\n",
    "    '.docx': 'docx',\n",
    "    '.csv': 'csv',\n",
    "    '.json': 'json',\n",
    "    '.jsonl': 'jsonl',\n",
    "}\n",
    "\n",
    "def scan_raw_files(raw_dir: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for path in sorted(raw_dir.rglob('*')):\n",
    "        if not path.is_file():\n",
    "            continue\n",
    "        ext = path.suffix.lower()\n",
    "        if ext in SUPPORTED_EXTS:\n",
    "            rows.append(\n",
    "                {\n",
    "                    'path': str(path),\n",
    "                    'ext': ext,\n",
    "                    'fmt': SUPPORTED_EXTS[ext],\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "raw_files_df = scan_raw_files(RAW_DIR)\n",
    "print('Найдено файлов:', len(raw_files_df))\n",
    "raw_files_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a324ae",
   "metadata": {},
   "source": [
    "## Block 3. Загрузка TXT / MD (plain text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(path: Path, encoding: str = 'utf-8') -> str:\n",
    "    with open(path, 'r', encoding=encoding, errors='ignore') as f:\n",
    "        return f.read()\n",
    "\n",
    "def convert_txt_md_to_docs(raw_files_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    mask = raw_files_df['fmt'].isin(['txt', 'md'])\n",
    "    subset = raw_files_df[mask].reset_index(drop=True)\n",
    "    for i, r in subset.iterrows():\n",
    "        path = Path(r['path'])\n",
    "        fmt = r['fmt']\n",
    "        text = load_text_file(path)\n",
    "        rows.append(\n",
    "            {\n",
    "                'doc_id': None,  # проставим позже\n",
    "                'source': 'raw_text',\n",
    "                'path': str(path),\n",
    "                'fmt': fmt,\n",
    "                'title': path.stem,\n",
    "                'section': None,\n",
    "                'page': None,\n",
    "                'text': text,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "docs_txt_md = convert_txt_md_to_docs(raw_files_df)\n",
    "print('TXT/MD docs:', docs_txt_md.shape)\n",
    "docs_txt_md.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfb3db8",
   "metadata": {},
   "source": [
    "## Block 4. Загрузка PDF: по целому файлу и по страницам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80fadf7",
   "metadata": {},
   "source": [
    "PDF можно обрабатывать по-разному:\n",
    "\n",
    "- **Целый документ как один текст** — проще, но тяжёлые файлы → длинные строки;\n",
    "- **По страницам** — удобно для ссылок вида «стр. 12» и для построения ссылок в сабмите.\n",
    "\n",
    "В этом блоке делаем два варианта: `mode='doc'` и `mode='page'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf_as_pages(path: Path) -> List[Dict[str, Any]]:\n",
    "    reader = PdfReader(str(path))\n",
    "    docs = []\n",
    "    for page_idx, page in enumerate(reader.pages):\n",
    "        try:\n",
    "            text = page.extract_text() or ''\n",
    "        except Exception:\n",
    "            text = ''\n",
    "        docs.append(\n",
    "            {\n",
    "                'source': 'pdf_page',\n",
    "                'path': str(path),\n",
    "                'fmt': 'pdf',\n",
    "                'title': path.stem,\n",
    "                'section': None,\n",
    "                'page': page_idx + 1,\n",
    "                'text': text,\n",
    "            }\n",
    "        )\n",
    "    return docs\n",
    "\n",
    "def convert_pdf_to_docs(raw_files_df: pd.DataFrame, mode: str = 'page') -> pd.DataFrame:\n",
    "    rows = []\n",
    "    subset = raw_files_df[raw_files_df['fmt'] == 'pdf'].reset_index(drop=True)\n",
    "    for _, r in tqdm(subset.iterrows(), total=len(subset), desc='PDF → docs'):\n",
    "        path = Path(r['path'])\n",
    "        if mode == 'page':\n",
    "            rows.extend(load_pdf_as_pages(path))\n",
    "        else:\n",
    "            # один документ на весь файл\n",
    "            pages = load_pdf_as_pages(path)\n",
    "            full_text = '\\n\\n'.join([p['text'] for p in pages])\n",
    "            rows.append(\n",
    "                {\n",
    "                    'source': 'pdf_doc',\n",
    "                    'path': str(path),\n",
    "                    'fmt': 'pdf',\n",
    "                    'title': path.stem,\n",
    "                    'section': None,\n",
    "                    'page': None,\n",
    "                    'text': full_text,\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "docs_pdf_pages = convert_pdf_to_docs(raw_files_df, mode='page')\n",
    "print('PDF docs (page mode):', docs_pdf_pages.shape)\n",
    "docs_pdf_pages.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a9c08",
   "metadata": {},
   "source": [
    "## Block 5. Загрузка HTML (зеркала документации, сайтов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b76aff1",
   "metadata": {},
   "source": [
    "Если в `raw_data/` лежат HTML-файлы (локальное зеркало сайта, документации), их можно превратить в текст с помощью `BeautifulSoup`.\n",
    "Важный момент — **убрать навигацию, меню, футеры**, если они сильно мешают."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd4bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4 lxml --quiet\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def load_html_as_text(path: Path) -> str:\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    # простой вариант: забрать только текст\n",
    "    text = soup.get_text(separator='\\n')\n",
    "    return text\n",
    "\n",
    "def convert_html_to_docs(raw_files_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    subset = raw_files_df[raw_files_df['fmt'] == 'html'].reset_index(drop=True)\n",
    "    for _, r in tqdm(subset.iterrows(), total=len(subset), desc='HTML → docs'):\n",
    "        path = Path(r['path'])\n",
    "        text = load_html_as_text(path)\n",
    "        rows.append(\n",
    "            {\n",
    "                'source': 'html',\n",
    "                'path': str(path),\n",
    "                'fmt': 'html',\n",
    "                'title': path.stem,\n",
    "                'section': None,\n",
    "                'page': None,\n",
    "                'text': text,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "docs_html = convert_html_to_docs(raw_files_df)\n",
    "print('HTML docs:', docs_html.shape)\n",
    "docs_html.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788b5ff",
   "metadata": {},
   "source": [
    "## Block 6. Загрузка DOCX (опционально)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560dfe54",
   "metadata": {},
   "source": [
    "Если в задаче под RAG могут быть `.docx`-файлы (отчёты, регламенты), их можно читать через `python-docx`.\n",
    "Ниже — пример, можно отключить, если библиотека недоступна в среде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-docx --quiet\n",
    "try:\n",
    "    import docx\n",
    "    DOCX_AVAILABLE = True\n",
    "except Exception:\n",
    "    DOCX_AVAILABLE = False\n",
    "    print('python-docx не установлен, блок DOCX будет пропущен')\n",
    "\n",
    "def load_docx_as_text(path: Path) -> str:\n",
    "    if not DOCX_AVAILABLE:\n",
    "        return ''\n",
    "    doc = docx.Document(str(path))\n",
    "    paras = [p.text for p in doc.paragraphs]\n",
    "    return '\\n'.join(paras)\n",
    "\n",
    "def convert_docx_to_docs(raw_files_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not DOCX_AVAILABLE:\n",
    "        return pd.DataFrame(columns=['doc_id', 'source', 'path', 'fmt', 'title', 'section', 'page', 'text'])\n",
    "    rows = []\n",
    "    subset = raw_files_df[raw_files_df['fmt'] == 'docx'].reset_index(drop=True)\n",
    "    for _, r in tqdm(subset.iterrows(), total=len(subset), desc='DOCX → docs'):\n",
    "        path = Path(r['path'])\n",
    "        text = load_docx_as_text(path)\n",
    "        rows.append(\n",
    "            {\n",
    "                'source': 'docx',\n",
    "                'path': str(path),\n",
    "                'fmt': 'docx',\n",
    "                'title': path.stem,\n",
    "                'section': None,\n",
    "                'page': None,\n",
    "                'text': text,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "docs_docx = convert_docx_to_docs(raw_files_df)\n",
    "print('DOCX docs:', docs_docx.shape)\n",
    "docs_docx.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a6f51b",
   "metadata": {},
   "source": [
    "## Block 7. CSV / JSON: табличные и структурированные данные\n",
    "\n",
    "Часто в задачах RAG есть таблицы:\n",
    "\n",
    "- CSV с фактами (например, `id, name, description, category, ...`);\n",
    "- JSON/JSONL с готовыми QA-парами (`question`, `answer`); \n",
    "- JSON, где лежат документы в виде массива `{id, title, text, ...}`.\n",
    "\n",
    "Можно превратить строки таблиц в отдельные документа-строчки RAG-корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed95e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_docs(raw_files_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    subset = raw_files_df[raw_files_df['fmt'] == 'csv'].reset_index(drop=True)\n",
    "    for _, r in tqdm(subset.iterrows(), total=len(subset), desc='CSV → docs'):\n",
    "        path = Path(r['path'])\n",
    "        df = pd.read_csv(path)\n",
    "        # пример: берем все текстовые колонки и склеиваем их в один текст\n",
    "        text_cols = [c for c in df.columns if df[c].dtype == 'object']\n",
    "        if not text_cols:\n",
    "            continue\n",
    "        for ridx, row in df.iterrows():\n",
    "            parts = []\n",
    "            for col in text_cols:\n",
    "                val = str(row[col])\n",
    "                if val and val != 'nan':\n",
    "                    parts.append(f'{col}: {val}')\n",
    "            text = '\\n'.join(parts)\n",
    "            rows.append(\n",
    "                {\n",
    "                    'source': 'csv_row',\n",
    "                    'path': str(path),\n",
    "                    'fmt': 'csv',\n",
    "                    'title': path.stem,\n",
    "                    'section': None,\n",
    "                    'page': None,\n",
    "                    'text': text,\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def convert_json_to_docs(raw_files_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    subset = raw_files_df[raw_files_df['fmt'].isin(['json', 'jsonl'])].reset_index(drop=True)\n",
    "    for _, r in tqdm(subset.iterrows(), total=len(subset), desc='JSON → docs'):\n",
    "        path = Path(r['path'])\n",
    "        if r['fmt'] == 'jsonl':\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    obj = json.loads(line)\n",
    "                    # простой вариант: взять question + answer, если есть\n",
    "                    question = str(obj.get('question', ''))\n",
    "                    answer = str(obj.get('answer', ''))\n",
    "                    if question or answer:\n",
    "                        text = f'Q: {question}\\nA: {answer}'\n",
    "                    else:\n",
    "                        text = json.dumps(obj, ensure_ascii=False)\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            'source': 'jsonl',\n",
    "                            'path': str(path),\n",
    "                            'fmt': 'jsonl',\n",
    "                            'title': path.stem,\n",
    "                            'section': None,\n",
    "                            'page': None,\n",
    "                            'text': text,\n",
    "                        }\n",
    "                    )\n",
    "        else:\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                obj = json.load(f)\n",
    "            # если это список объектов\n",
    "            if isinstance(obj, list):\n",
    "                for item in obj:\n",
    "                    if isinstance(item, dict):\n",
    "                        question = str(item.get('question', ''))\n",
    "                        answer = str(item.get('answer', ''))\n",
    "                        if question or answer:\n",
    "                            text = f'Q: {question}\\nA: {answer}'\n",
    "                        else:\n",
    "                            text = json.dumps(item, ensure_ascii=False)\n",
    "                    else:\n",
    "                        text = str(item)\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            'source': 'json',\n",
    "                            'path': str(path),\n",
    "                            'fmt': 'json',\n",
    "                            'title': path.stem,\n",
    "                            'section': None,\n",
    "                            'page': None,\n",
    "                            'text': text,\n",
    "                        }\n",
    "                    )\n",
    "            else:\n",
    "                # один объект\n",
    "                text = json.dumps(obj, ensure_ascii=False)\n",
    "                rows.append(\n",
    "                    {\n",
    "                        'source': 'json',\n",
    "                        'path': str(path),\n",
    "                        'fmt': 'json',\n",
    "                        'title': path.stem,\n",
    "                        'section': None,\n",
    "                        'page': None,\n",
    "                        'text': text,\n",
    "                    }\n",
    "                )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "docs_csv = convert_csv_to_docs(raw_files_df)\n",
    "docs_json = convert_json_to_docs(raw_files_df)\n",
    "print('CSV docs :', docs_csv.shape)\n",
    "print('JSON docs:', docs_json.shape)\n",
    "docs_csv.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71503d6",
   "metadata": {},
   "source": [
    "## Block 8. Объединяем всё в единый `docs_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1595a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = [\n",
    "    docs_txt_md,\n",
    "    docs_pdf_pages,\n",
    "    docs_html,\n",
    "    docs_docx,\n",
    "    docs_csv,\n",
    "    docs_json,\n",
    "]\n",
    "docs_df = pd.concat([d for d in all_docs if d is not None and len(d) > 0], ignore_index=True)\n",
    "docs_df.insert(0, 'doc_id', np.arange(len(docs_df)))\n",
    "print('docs_df shape:', docs_df.shape)\n",
    "docs_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f7f20",
   "metadata": {},
   "source": [
    "## Block 9. Быстрая диагностика корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5107583",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df['n_chars'] = docs_df['text'].astype(str).str.len()\n",
    "print(docs_df['n_chars'].describe())\n",
    "print('\\nФорматы документов:')\n",
    "print(docs_df['fmt'].value_counts())\n",
    "print('\\nИсточники:')\n",
    "print(docs_df['source'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e85c23",
   "metadata": {},
   "source": [
    "## Block 10. Чанкинг: по символам, по предложениям, гибрид\n",
    "\n",
    "После того как у нас есть `docs_df`, обычно делаем **ещё один уровень разбиения** — на чанки.\n",
    "\n",
    "Самые популярные варианты:\n",
    "\n",
    "1. **Char-based** (простой как в финальном ноуте):\n",
    "   - фиксированный `CHUNK_SIZE` по символам;\n",
    "   - overlap `CHUNK_OVERLAP`.\n",
    "\n",
    "2. **Sentence-based**:\n",
    "   - разбиваем текст на предложения (nltk/spacy);\n",
    "   - собираем из них чанки \"по количеству токенов/символов\".\n",
    "\n",
    "3. **Гибрид**:\n",
    "   - учитываем границы страниц/разделов/заголовков;\n",
    "   - внутри уже используем символы/предложения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538624a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "def split_text_char_based(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    chunks = []\n",
    "    if not text:\n",
    "        return chunks\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "    while i < n:\n",
    "        chunk = text[i : i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "        i += step\n",
    "    return chunks\n",
    "\n",
    "chunks_rows = []\n",
    "for _, row in tqdm(docs_df.iterrows(), total=len(docs_df), desc='Char-based chunking'):\n",
    "    doc_id = int(row['doc_id'])\n",
    "    text = str(row['text'])\n",
    "    chunks = split_text_char_based(text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "    for idx, ch in enumerate(chunks):\n",
    "        chunks_rows.append(\n",
    "            {\n",
    "                'chunk_id': None,\n",
    "                'doc_id': doc_id,\n",
    "                'chunk_idx_in_doc': idx,\n",
    "                'text': ch,\n",
    "                'source': row['source'],\n",
    "                'path': row['path'],\n",
    "                'fmt': row['fmt'],\n",
    "                'title': row['title'],\n",
    "                'section': row['section'],\n",
    "                'page': row['page'],\n",
    "            }\n",
    "        )\n",
    "\n",
    "chunks_df = pd.DataFrame(chunks_rows)\n",
    "chunks_df.insert(0, 'chunk_id', np.arange(len(chunks_df)))\n",
    "chunks_df['n_chars'] = chunks_df['text'].str.len()\n",
    "print('chunks_df shape:', chunks_df.shape)\n",
    "print(chunks_df[['chunk_id', 'doc_id', 'chunk_idx_in_doc', 'n_chars']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d89aebf",
   "metadata": {},
   "source": [
    "### (Опционально) Sentence-based чанкинг через `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0441c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def split_text_sentence_based(text: str, max_chars: int = 1000) -> List[str]:\n",
    "    sents = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    cur = []\n",
    "    cur_len = 0\n",
    "    for s in sents:\n",
    "        s = s.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        if cur_len + len(s) + 1 > max_chars and cur:\n",
    "            chunks.append(' '.join(cur))\n",
    "            cur = [s]\n",
    "            cur_len = len(s)\n",
    "        else:\n",
    "            cur.append(s)\n",
    "            cur_len += len(s) + 1\n",
    "    if cur:\n",
    "        chunks.append(' '.join(cur))\n",
    "    return chunks\n",
    "\n",
    "# пример: sentence-based чанки для одного документа\n",
    "example_doc = docs_df.iloc[0]\n",
    "sent_chunks = split_text_sentence_based(str(example_doc['text'])[:3000], max_chars=400)\n",
    "print('Документ:', example_doc['title'])\n",
    "print('Количество sentence-based чанков:', len(sent_chunks))\n",
    "for i, ch in enumerate(sent_chunks[:3]):\n",
    "    print(f\"--- chunk {i} ---\")\n",
    "    print(ch)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff1472",
   "metadata": {},
   "source": [
    "## Block 12. Сохранение корпуса и чанков для RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae8956",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_path_parquet = PROC_DIR / 'docs.parquet'\n",
    "chunks_path_parquet = PROC_DIR / 'chunks.parquet'\n",
    "docs_path_csv = PROC_DIR / 'docs.csv'\n",
    "chunks_path_csv = PROC_DIR / 'chunks.csv'\n",
    "\n",
    "docs_df.to_parquet(docs_path_parquet, index=False)\n",
    "chunks_df.to_parquet(chunks_path_parquet, index=False)\n",
    "docs_df.to_csv(docs_path_csv, index=False)\n",
    "chunks_df.to_csv(chunks_path_csv, index=False)\n",
    "\n",
    "print('Сохранено:')\n",
    "print('  ', docs_path_parquet)\n",
    "print('  ', chunks_path_parquet)\n",
    "print('  ', docs_path_csv)\n",
    "print('  ', chunks_path_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e1af3b",
   "metadata": {},
   "source": [
    "## Итог\n",
    "\n",
    "В этом ноутбуке ты получил полный цикл **подготовки данных для RAG**:\n",
    "\n",
    "1. Собрали разные форматы (PDF, TXT/MD, HTML, DOCX, CSV, JSON/JSONL) из `raw_data/`.\n",
    "2. Привели всё к единому табличному формату `docs_df`.\n",
    "3. Сделали char-based и sentence-based чанкинг → `chunks_df`.\n",
    "4. Сохранили результат в `processed/`.\n",
    "\n",
    "Дальше любой RAG-бейзлайн может просто делать:\n",
    "\n",
    "```python\n",
    "docs_df   = pd.read_parquet('processed/docs.parquet')\n",
    "chunks_df = pd.read_parquet('processed/chunks.parquet')\n",
    "```\n",
    "\n",
    "и уже работать только с этими таблицами (эмбеддинги, индексы, retriever, LLM)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}