{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2172354a",
   "metadata": {},
   "source": [
    "# RAG Submissions Blender (Auto-tuning)\n",
    "\n",
    "Ноутбук для **автоматического подбора правил ансамблирования (blending)** RAG-сабмитов по локальному ground truth.\n",
    "\n",
    "Идея:\n",
    "\n",
    "- есть два сабмита `submission_1.csv` и `submission_2.csv`;\n",
    "- есть `ground_truth.csv` с колонками `id` и `answer`;\n",
    "- мы перебираем набор правил (пороги длины, похожести, дефолтный источник) и\n",
    "  выбираем конфиг, который даёт **максимальный mean token-F1** на ground truth;\n",
    "- затем этим лучшим конфигом строим финальный `submission_blended.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d7c19",
   "metadata": {},
   "source": [
    "## Block 0. Конфиг и импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c363d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import difflib\n",
    "import re\n",
    "\n",
    "BASE_DIR = Path('.').resolve()\n",
    "INPUT_DIR = BASE_DIR / 'inputs'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "\n",
    "INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Пути к сабмитам и ground truth\n",
    "SUB1_PATH = INPUT_DIR / 'submission_1.csv'\n",
    "SUB2_PATH = INPUT_DIR / 'submission_2.csv'\n",
    "GT_PATH   = INPUT_DIR / 'ground_truth.csv'\n",
    "\n",
    "# Конфигурация колонок\n",
    "ID_COL = 'id'\n",
    "ANSWER_COL = 'answer'\n",
    "REFS_COL = 'refs_json'  # опционально\n",
    "\n",
    "print('BASE_DIR  :', BASE_DIR)\n",
    "print('INPUT_DIR :', INPUT_DIR)\n",
    "print('OUTPUT_DIR:', OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086d3fe",
   "metadata": {},
   "source": [
    "## Block 1. Загрузка сабмитов и ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda31ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_submission(path: Path, name: str) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Не найден сабмит {name}: {path}')\n",
    "    df = pd.read_csv(path)\n",
    "    print(f'{name}: shape =', df.shape)\n",
    "    if ID_COL not in df.columns or ANSWER_COL not in df.columns:\n",
    "        raise KeyError(f'В сабмите {name} должны быть колонки {ID_COL!r} и {ANSWER_COL!r}')\n",
    "    return df\n",
    "\n",
    "sub1 = load_submission(SUB1_PATH, 'sub1')\n",
    "sub2 = load_submission(SUB2_PATH, 'sub2')\n",
    "\n",
    "if not GT_PATH.exists():\n",
    "    raise FileNotFoundError(f'Не найден ground truth: {GT_PATH}')\n",
    "gt = pd.read_csv(GT_PATH)\n",
    "print('ground truth shape =', gt.shape)\n",
    "if ID_COL not in gt.columns or ANSWER_COL not in gt.columns:\n",
    "    raise KeyError(f'В ground truth должны быть колонки {ID_COL!r} и {ANSWER_COL!r}')\n",
    "\n",
    "# ограничимся только общими id, которые есть во всех трёх таблицах\n",
    "ids1 = set(sub1[ID_COL])\n",
    "ids2 = set(sub2[ID_COL])\n",
    "ids_gt = set(gt[ID_COL])\n",
    "common_ids = sorted(list(ids1 & ids2 & ids_gt))\n",
    "print('\\nОбщих id во всех трёх:', len(common_ids))\n",
    "\n",
    "sub1c = sub1[sub1[ID_COL].isin(common_ids)].copy()\n",
    "sub2c = sub2[sub2[ID_COL].isin(common_ids)].copy()\n",
    "gtc   = gt[gt[ID_COL].isin(common_ids)].copy()\n",
    "\n",
    "sub1c.sort_values(ID_COL, inplace=True)\n",
    "sub2c.sort_values(ID_COL, inplace=True)\n",
    "gtc.sort_values(ID_COL, inplace=True)\n",
    "\n",
    "blend_df = pd.DataFrame({ID_COL: common_ids})\n",
    "blend_df['answer_1'] = sub1c[ANSWER_COL].astype(str).fillna('')\n",
    "blend_df['answer_2'] = sub2c[ANSWER_COL].astype(str).fillna('')\n",
    "blend_df['gt_answer'] = gtc[ANSWER_COL].astype(str).fillna('')\n",
    "\n",
    "if REFS_COL in sub1c.columns and REFS_COL in sub2c.columns:\n",
    "    blend_df['refs_1'] = sub1c[REFS_COL].astype(str).fillna('')\n",
    "    blend_df['refs_2'] = sub2c[REFS_COL].astype(str).fillna('')\n",
    "else:\n",
    "    blend_df['refs_1'] = ''\n",
    "    blend_df['refs_2'] = ''\n",
    "\n",
    "print('\\nblend_df preview:')\n",
    "blend_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd0c07",
   "metadata": {},
   "source": [
    "## Block 2. Вспомогательные функции: токенизация, F1, похожесть, валидность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    if text is None:\n",
    "        return []\n",
    "    return re.findall(r\"\\w+\", str(text).lower(), flags=re.UNICODE)\n",
    "\n",
    "def simple_f1(pred: str, gt: str) -> float:\n",
    "    pred_tokens = tokenize(pred)\n",
    "    gt_tokens = tokenize(gt)\n",
    "    if not pred_tokens or not gt_tokens:\n",
    "        return 0.0\n",
    "    common = set(pred_tokens) & set(gt_tokens)\n",
    "    if not common:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gt_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def simple_similarity(a: str, b: str) -> float:\n",
    "    a = (a or '').strip()\n",
    "    b = (b or '').strip()\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def jaccard_tokens(a: str, b: str) -> float:\n",
    "    ta = set(tokenize(a))\n",
    "    tb = set(tokenize(b))\n",
    "    if not ta and not tb:\n",
    "        return 1.0\n",
    "    if not ta or not tb:\n",
    "        return 0.0\n",
    "    inter = len(ta & tb)\n",
    "    union = len(ta | tb)\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return inter / union\n",
    "\n",
    "def is_invalid_answer(text: str) -> bool:\n",
    "    if text is None:\n",
    "        return True\n",
    "    t = str(text).strip()\n",
    "    if not t:\n",
    "        return True\n",
    "    lowered = t.lower()\n",
    "    if lowered.startswith('error:') or 'traceback' in lowered:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def answer_length(text: str) -> int:\n",
    "    if text is None:\n",
    "        return 0\n",
    "    return len(str(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970bdc2a",
   "metadata": {},
   "source": [
    "## Block 3. Правило blending, зависящее от параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf845ed0",
   "metadata": {},
   "source": [
    "Мы определяем функцию, которая выбирает ответ на основе набора **параметров**:\n",
    "\n",
    "- `min_len` — минимальная «нормальная» длина ответа;\n",
    "- `max_len` — максимальная разумная длина (простыни выше этого считаем подозрительными);\n",
    "- `sim_thr` — порог похожести для \"почти одинаковых\" ответов;\n",
    "- `default_src` — какой сабмит считать более сильным по умолчанию (`1` или `2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c687c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_two_answers_with_params(a1: str, a2: str, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    a1 = str(a1 or '')\n",
    "    a2 = str(a2 or '')\n",
    "\n",
    "    min_len = params.get('min_len', 15)\n",
    "    max_len = params.get('max_len', 800)\n",
    "    sim_thr = params.get('sim_thr', 0.9)\n",
    "    default_src = params.get('default_src', 2)  # 1 или 2\n",
    "\n",
    "    invalid_1 = is_invalid_answer(a1)\n",
    "    invalid_2 = is_invalid_answer(a2)\n",
    "\n",
    "    len1 = answer_length(a1)\n",
    "    len2 = answer_length(a2)\n",
    "\n",
    "    sim_seq = simple_similarity(a1, a2)\n",
    "    sim_jac = jaccard_tokens(a1, a2)\n",
    "\n",
    "    # 1) один невалидный → другой\n",
    "    if invalid_1 and not invalid_2:\n",
    "        return {'answer': a2, 'src': 2, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "    if invalid_2 and not invalid_1:\n",
    "        return {'answer': a1, 'src': 1, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "\n",
    "    # 2) оба невалидные → первый\n",
    "    if invalid_1 and invalid_2:\n",
    "        return {'answer': a1, 'src': 1, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "\n",
    "    # 3) короткие vs нормальные\n",
    "    if len1 < min_len <= len2:\n",
    "        return {'answer': a2, 'src': 2, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "    if len2 < min_len <= len1:\n",
    "        return {'answer': a1, 'src': 1, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "\n",
    "    # 4) слишком длинные\n",
    "    if len1 > max_len and len2 <= max_len:\n",
    "        return {'answer': a2, 'src': 2, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "    if len2 > max_len and len1 <= max_len:\n",
    "        return {'answer': a1, 'src': 1, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "\n",
    "    # 5) почти одинаковые → берём более короткий\n",
    "    if sim_seq >= sim_thr or sim_jac >= sim_thr:\n",
    "        if len1 <= len2:\n",
    "            return {'answer': a1, 'src': 1, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "        else:\n",
    "            return {'answer': a2, 'src': 2, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "\n",
    "    # 6) default: доверяем default_src\n",
    "    if default_src == 1:\n",
    "        return {'answer': a1, 'src': 1, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "    else:\n",
    "        return {'answer': a2, 'src': 2, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e71ec",
   "metadata": {},
   "source": [
    "## Block 4. Функция оценки качества для заданных параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fccce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_params(params: Dict[str, Any], sample_size: int = None) -> Dict[str, Any]:\n",
    "    df = blend_df\n",
    "    if sample_size is not None and sample_size < len(df):\n",
    "        df = df.sample(sample_size, random_state=42)\n",
    "\n",
    "    f1_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        a1 = row['answer_1']\n",
    "        a2 = row['answer_2']\n",
    "        gt = row['gt_answer']\n",
    "        res = blend_two_answers_with_params(a1, a2, params)\n",
    "        f1 = simple_f1(res['answer'], gt)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "    mean_f1 = float(np.mean(f1_list)) if f1_list else 0.0\n",
    "    out = params.copy()\n",
    "    out['mean_f1'] = mean_f1\n",
    "    out['n'] = len(df)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afb47f8",
   "metadata": {},
   "source": [
    "## Block 5. Перебор конфигураций (grid search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e556b",
   "metadata": {},
   "source": [
    "Сделаем простой grid search по:\n",
    "\n",
    "- `min_len`  ∈ {10, 20, 40};\n",
    "- `max_len`  ∈ {500, 800, 1200};\n",
    "- `sim_thr`  ∈ {0.85, 0.9, 0.95};\n",
    "- `default_src` ∈ {1, 2}.\n",
    "\n",
    "При желании можно расширить/сузить сетку. Для скорости можно задать `sample_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422fde7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len_options = [10, 20, 40]\n",
    "max_len_options = [500, 800, 1200]\n",
    "sim_thr_options = [0.85, 0.9, 0.95]\n",
    "default_src_options = [1, 2]\n",
    "\n",
    "# Можно ограничить размер выборки для быстрого подбора (None = вся выборка)\n",
    "SAMPLE_SIZE_FOR_SEARCH = None  # например, 200\n",
    "\n",
    "results = []\n",
    "for min_len in min_len_options:\n",
    "    for max_len in max_len_options:\n",
    "        for sim_thr in sim_thr_options:\n",
    "            for default_src in default_src_options:\n",
    "                params = {\n",
    "                    'min_len': min_len,\n",
    "                    'max_len': max_len,\n",
    "                    'sim_thr': sim_thr,\n",
    "                    'default_src': default_src,\n",
    "                }\n",
    "                res = evaluate_params(params, sample_size=SAMPLE_SIZE_FOR_SEARCH)\n",
    "                results.append(res)\n",
    "\n",
    "search_df = pd.DataFrame(results)\n",
    "search_df = search_df.sort_values('mean_f1', ascending=False).reset_index(drop=True)\n",
    "print('Лучшие конфиги по mean_f1:')\n",
    "search_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945b52f",
   "metadata": {},
   "source": [
    "## Block 6. Выбор лучшего конфига и blending на всех id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = search_df.iloc[0][['min_len', 'max_len', 'sim_thr', 'default_src']].to_dict()\n",
    "print('Лучший конфиг:', best_params)\n",
    "\n",
    "blend_results_full = []\n",
    "for _, row in tqdm(blend_df.iterrows(), total=len(blend_df), desc='Blending with best params'):\n",
    "    a1 = row['answer_1']\n",
    "    a2 = row['answer_2']\n",
    "    res = blend_two_answers_with_params(a1, a2, best_params)\n",
    "    blend_results_full.append(res)\n",
    "\n",
    "blend_meta_full = pd.DataFrame(blend_results_full)\n",
    "blend_meta_full.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7acb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = blend_df[[ID_COL]].copy()\n",
    "final_df[ANSWER_COL] = blend_meta_full['answer']\n",
    "final_df['src'] = blend_meta_full['src']\n",
    "final_df['sim_seq'] = blend_meta_full['sim_seq']\n",
    "final_df['sim_jac'] = blend_meta_full['sim_jac']\n",
    "\n",
    "# выбираем refs_json в зависимости от src\n",
    "final_refs = []\n",
    "for i, row in final_df.iterrows():\n",
    "    src = row['src']\n",
    "    if src == 1:\n",
    "        final_refs.append(blend_df.loc[i, 'refs_1'])\n",
    "    elif src == 2:\n",
    "        final_refs.append(blend_df.loc[i, 'refs_2'])\n",
    "    else:\n",
    "        final_refs.append('')\n",
    "\n",
    "final_df[REFS_COL] = final_refs\n",
    "\n",
    "print('final_df shape:', final_df.shape)\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae840d5d",
   "metadata": {},
   "source": [
    "## Block 7. Сравнение F1 сабмитов и blended-версии на всём ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dcc418",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_1_all = []\n",
    "f1_2_all = []\n",
    "f1_b_all = []\n",
    "\n",
    "for _, row in blend_df.iterrows():\n",
    "    gt_ans = row['gt_answer']\n",
    "    f1_1_all.append(simple_f1(row['answer_1'], gt_ans))\n",
    "    f1_2_all.append(simple_f1(row['answer_2'], gt_ans))\n",
    "\n",
    "# final_df соответствует blend_df по индексу\n",
    "for i, row in final_df.iterrows():\n",
    "    gt_ans = blend_df.loc[i, 'gt_answer']\n",
    "    f1_b_all.append(simple_f1(row[ANSWER_COL], gt_ans))\n",
    "\n",
    "print('=== Итоговая оценка по token-F1 (на общих id) ===')\n",
    "print('sub1   mean F1:', float(np.mean(f1_1_all)))\n",
    "print('sub2   mean F1:', float(np.mean(f1_2_all)))\n",
    "print('blend  mean F1:', float(np.mean(f1_b_all)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ffb97",
   "metadata": {},
   "source": [
    "## Block 8. Сохранение blended-сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd4820",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEND_PATH = OUTPUT_DIR / 'submission_blended_auto.csv'\n",
    "\n",
    "cols_for_submit = [ID_COL, ANSWER_COL]\n",
    "if REFS_COL in final_df.columns:\n",
    "    cols_for_submit.append(REFS_COL)\n",
    "\n",
    "submit_blend = final_df[cols_for_submit].copy()\n",
    "submit_blend.to_csv(BLEND_PATH, index=False)\n",
    "print('Авто-бленд сабмит сохранён в:', BLEND_PATH)\n",
    "submit_blend.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c76f68",
   "metadata": {},
   "source": [
    "## Итог\n",
    "\n",
    "В этом ноутбуке:\n",
    "\n",
    "1. Загрузили два сабмита и ground truth.\n",
    "2. Привели их к единой таблице `blend_df` с `answer_1`, `answer_2`, `gt_answer`.\n",
    "3. Определили параметризованное правило blending (`min_len`, `max_len`, `sim_thr`, `default_src`).\n",
    "4. Сделали grid search по конфигам и выбрали лучший по mean token-F1.\n",
    "5. Этим конфигом собрали финальный blended-сабмит.\n",
    "6. Сравнили F1 `sub1`, `sub2` и blended-версии.\n",
    "7. Сохранили итоговый `submission_blended_auto.csv`.\n",
    "\n",
    "На финале можно:\n",
    "- запускать этот ноутбук на валидационной части (где есть GT) для подбора параметров;\n",
    "- переносить найденный конфиг (значения параметров) в основной блендер, который работает уже без GT."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}