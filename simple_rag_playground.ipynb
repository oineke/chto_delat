{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae1d166",
   "metadata": {},
   "source": [
    "# Simple RAG Playground\n",
    "\n",
    "Этот ноутбук — лёгкий, автономный playground для RAG:\n",
    "- без Qdrant, всё в памяти;\n",
    "- dense + BM25 + гибридный ретривер;\n",
    "- отладка качества: смотрим, какие куски текста достаются под вопрос;\n",
    "- простая оффлайн-оценка, если есть столбец с правильным ответом.\n",
    "\n",
    "Структура блоков:\n",
    "0) Setup & конфиг\n",
    "1) Загрузка данных (docs + QA)\n",
    "2) Chunking\n",
    "3) Эмбеддинги + in-memory индекс + BM25\n",
    "4) Ретраиверы + гибрид\n",
    "5) LLM-генерация (локальная HF-модель)\n",
    "6) Debug для одного вопроса\n",
    "7) Оффлайн-оценка на подвыборке\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90745cb1",
   "metadata": {},
   "source": [
    "## Block 0. Setup & конфиг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0195207",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers rank-bm25 datasets nltk --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0152ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "\n",
    "# === Конфиг данных ===\n",
    "DATA_DIR = Path('/kaggle/input')  # поменяй под себя\n",
    "DOC_SOURCE = 'hf'  # 'hf' или 'local'\n",
    "\n",
    "# HF dataset (например, если в нём есть тексты и QA)\n",
    "HF_DATASET_NAME = 'your/dataset'   # замени\n",
    "HF_SPLIT_DOCS = 'train'\n",
    "HF_SPLIT_QA = 'validation'\n",
    "HF_DOC_TEXT_COL = 'content'        # колонка с текстом в HF\n",
    "\n",
    "# Локальный вариант (если DOC_SOURCE='local')\n",
    "LOCAL_DOCS_CSV = DATA_DIR / 'docs.csv'  # docs.csv: id, text\n",
    "LOCAL_QA_CSV = DATA_DIR / 'qa.csv'      # qa.csv: id, question, [answer]\n",
    "LOCAL_DOC_ID_COL = 'id'\n",
    "LOCAL_DOC_TEXT_COL = 'text'\n",
    "\n",
    "# === Chunking ===\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 200\n",
    "MIN_CHARS = 50\n",
    "\n",
    "# === Retrieval ===\n",
    "TOP_K_DENSE = 15\n",
    "TOP_K_BM25 = 15\n",
    "TOP_K_FINAL = 8\n",
    "\n",
    "# === Эмбеддер + LLM ===\n",
    "EMBED_MODEL_ID = 'intfloat/multilingual-e5-large'\n",
    "GEN_MODEL_PATH = '/kaggle/input/qwen2.5/transformers/1.5b-instruct/1'  # замени при необходимости\n",
    "MAX_NEW_TOKENS = 256\n",
    "TEMP = 0.2\n",
    "TOP_P = 0.9\n",
    "\n",
    "# === Колонки QA/submit ===\n",
    "QA_ID_COL = 'id'\n",
    "QA_QUESTION_COL = 'question'\n",
    "QA_GT_ANSWER_COL = 'answer'  # если нет GT, можно игнорировать\n",
    "\n",
    "SUBM_ID_COL = 'id'\n",
    "SUBM_ANSWER_COL = 'answer'\n",
    "SUBM_REFS_COL = 'refs_json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d4ae1d",
   "metadata": {},
   "source": [
    "## Block 1. Загрузка данных (docs + QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95308bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs_and_qa() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if DOC_SOURCE == 'hf':\n",
    "        ds_docs = load_dataset(HF_DATASET_NAME, split=HF_SPLIT_DOCS)\n",
    "        docs_df = ds_docs.to_pandas()\n",
    "        ds_qa = load_dataset(HF_DATASET_NAME, split=HF_SPLIT_QA)\n",
    "        qa_df = ds_qa.to_pandas()\n",
    "    else:\n",
    "        docs_df = pd.read_csv(LOCAL_DOCS_CSV)\n",
    "        qa_df = pd.read_csv(LOCAL_QA_CSV)\n",
    "\n",
    "    print('Docs shape:', docs_df.shape)\n",
    "    print('QA shape:', qa_df.shape)\n",
    "    return docs_df, qa_df\n",
    "\n",
    "docs_df, qa_df = load_docs_and_qa()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629346b",
   "metadata": {},
   "source": [
    "## Block 2. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3113d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def make_chunks_from_docs(\n",
    "    docs_df: pd.DataFrame,\n",
    "    text_col: str,\n",
    "    id_col: str = 'doc_id',\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "    chunk_overlap: int = CHUNK_OVERLAP,\n",
    "    min_chars: int = MIN_CHARS,\n",
    ") -> pd.DataFrame:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=['\\n\\n', '\\n', '. ', ' ', ''],\n",
    "    )\n",
    "    rows = []\n",
    "    for _, row in tqdm(docs_df.iterrows(), total=len(docs_df), desc='Chunking docs'):\n",
    "        doc_id = row[id_col] if id_col in docs_df.columns else _\n",
    "        text = str(row[text_col])\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        parts = splitter.split_text(text)\n",
    "        for idx, ch in enumerate(parts):\n",
    "            ch = ch.strip()\n",
    "            if len(ch) < min_chars:\n",
    "                continue\n",
    "            rows.append({'doc_id': doc_id, 'chunk_id': f'{doc_id}_{idx}', 'text': ch})\n",
    "    chunks_df = pd.DataFrame(rows)\n",
    "    print('Total chunks:', chunks_df.shape)\n",
    "    return chunks_df\n",
    "\n",
    "if DOC_SOURCE == 'hf':\n",
    "    chunks_df = make_chunks_from_docs(docs_df, text_col=HF_DOC_TEXT_COL, id_col='id' if 'id' in docs_df.columns else 'doc_id')\n",
    "else:\n",
    "    chunks_df = make_chunks_from_docs(docs_df, text_col=LOCAL_DOC_TEXT_COL, id_col=LOCAL_DOC_ID_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec25440",
   "metadata": {},
   "source": [
    "## Block 3. Эмбеддинги + in-memory индекс + BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(EMBED_MODEL_ID, device=str(device))\n",
    "embedding_model.eval()\n",
    "print('Loaded embedder:', EMBED_MODEL_ID)\n",
    "\n",
    "def build_dense_index(chunks_df: pd.DataFrame) -> np.ndarray:\n",
    "    texts = chunks_df['text'].tolist()\n",
    "    vecs = embedding_model.encode(\n",
    "        texts,\n",
    "        batch_size=64,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    return np.array(vecs, dtype=np.float32)\n",
    "\n",
    "dense_matrix = build_dense_index(chunks_df)\n",
    "print('dense_matrix shape:', dense_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_chunks = [word_tokenize(t) for t in chunks_df['text'].tolist()]\n",
    "bm25 = BM25Okapi(tokenized_chunks)\n",
    "print('BM25 corpus size:', len(tokenized_chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056b0d8",
   "metadata": {},
   "source": [
    "## Block 4. Ретраиверы (dense / BM25 / hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2308f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_search(query: str, top_k: int = TOP_K_DENSE) -> pd.DataFrame:\n",
    "    q_vec = embedding_model.encode([query], normalize_embeddings=True)\n",
    "    q_vec = q_vec.astype(np.float32)[0]\n",
    "    sims = dense_matrix @ q_vec\n",
    "    idx = np.argsort(-sims)[:top_k]\n",
    "    sub = chunks_df.iloc[idx].copy()\n",
    "    sub['score_dense'] = sims[idx]\n",
    "    return sub\n",
    "\n",
    "def bm25_search(query: str, top_k: int = TOP_K_BM25) -> pd.DataFrame:\n",
    "    toks = word_tokenize(query)\n",
    "    scores = bm25.get_scores(toks)\n",
    "    idx = np.argsort(-scores)[:top_k]\n",
    "    sub = chunks_df.iloc[idx].copy()\n",
    "    sub['score_bm25'] = scores[idx]\n",
    "    return sub\n",
    "\n",
    "def hybrid_search(query: str, alpha: float = 0.5, top_k: int = TOP_K_FINAL) -> pd.DataFrame:\n",
    "    d = dense_search(query, top_k=TOP_K_DENSE)\n",
    "    b = bm25_search(query, top_k=TOP_K_BM25)\n",
    "\n",
    "    d_ = d[['doc_id', 'chunk_id', 'score_dense']]\n",
    "    b_ = b[['doc_id', 'chunk_id', 'score_bm25']]\n",
    "    merged = pd.merge(d_, b_, on=['doc_id', 'chunk_id'], how='outer')\n",
    "\n",
    "    max_d = merged['score_dense'].max() if merged['score_dense'].notna().any() else 1.0\n",
    "    max_b = merged['score_bm25'].max() if merged['score_bm25'].notna().any() else 1.0\n",
    "    merged['score_dense_n'] = merged['score_dense'] / max_d\n",
    "    merged['score_bm25_n'] = merged['score_bm25'] / max_b\n",
    "    merged['score_hybrid'] = alpha * merged['score_dense_n'].fillna(0) + (1 - alpha) * merged['score_bm25_n'].fillna(0)\n",
    "\n",
    "    out = pd.merge(merged, chunks_df, on=['doc_id', 'chunk_id'], how='left')\n",
    "    out = out.sort_values('score_hybrid', ascending=False).head(top_k).reset_index(drop=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beea8d2",
   "metadata": {},
   "source": [
    "## Block 5. LLM-генерация (Qwen / любая HF-модель)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pipeline = pipeline(\n",
    "    'text-generation',\n",
    "    model=GEN_MODEL_PATH,\n",
    "    device=0 if device.type == 'cuda' else -1,\n",
    "    torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
    ")\n",
    "print('Loaded generation model from:', GEN_MODEL_PATH)\n",
    "\n",
    "SYSTEM_PROMPT = '''You are a helpful assistant that answers questions using the provided context.\n",
    "Answer in the same language as the question.\n",
    "If the answer cannot be found in the context, say that you don't know and avoid hallucinations.\n",
    "'''\n",
    "\n",
    "def build_prompt(query: str, context_chunks: List[str]) -> str:\n",
    "    ctx = '\\n\\n'.join(context_chunks)\n",
    "    return f\"{SYSTEM_PROMPT}\\n\\nContext:\\n{ctx}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\"\n",
    "\n",
    "def llm_answer(query: str, context_chunks: List[str]) -> str:\n",
    "    prompt = build_prompt(query, context_chunks)\n",
    "    out = generation_pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=TEMP,\n",
    "        top_p=TOP_P,\n",
    "        pad_token_id=generation_pipeline.tokenizer.eos_token_id,\n",
    "    )\n",
    "    text = out[0]['generated_text']\n",
    "    if 'Answer:' in text:\n",
    "        text = text.split('Answer:', 1)[1].strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb075d",
   "metadata": {},
   "source": [
    "## Block 6. Debug одного вопроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_question(\n",
    "    query: str,\n",
    "    mode: str = 'hybrid',  # 'dense', 'bm25', 'hybrid'\n",
    "    top_k: int = TOP_K_FINAL,\n",
    ") -> Dict[str, Any]:\n",
    "    if mode == 'dense':\n",
    "        retrieved = dense_search(query, top_k=top_k)\n",
    "    elif mode == 'bm25':\n",
    "        retrieved = bm25_search(query, top_k=top_k)\n",
    "    else:\n",
    "        retrieved = hybrid_search(query, top_k=top_k)\n",
    "\n",
    "    context_chunks = retrieved['text'].tolist()\n",
    "    answer = llm_answer(query, context_chunks)\n",
    "\n",
    "    print('=== QUESTION ===')\n",
    "    print(query)\n",
    "    print('\\n=== TOP CHUNKS ===')\n",
    "    for i, row in retrieved.head(top_k).iterrows():\n",
    "        print(f\"--- chunk #{i} | doc_id={row['doc_id']} | chunk_id={row['chunk_id']}\")\n",
    "        if 'score_hybrid' in row and not pd.isna(row['score_hybrid']):\n",
    "            print('score_hybrid =', row['score_hybrid'])\n",
    "        elif 'score_dense' in row and not pd.isna(row['score_dense']):\n",
    "            print('score_dense =', row['score_dense'])\n",
    "        elif 'score_bm25' in row and not pd.isna(row['score_bm25']):\n",
    "            print('score_bm25 =', row['score_bm25'])\n",
    "        print(row['text'][:600])\n",
    "        print('')\n",
    "\n",
    "    print('=== ANSWER ===')\n",
    "    print(answer)\n",
    "\n",
    "    refs = [\n",
    "        {\n",
    "            'doc_id': row['doc_id'],\n",
    "            'chunk_id': row['chunk_id'],\n",
    "            'score_hybrid': row.get('score_hybrid', None),\n",
    "            'score_dense': row.get('score_dense', None),\n",
    "            'score_bm25': row.get('score_bm25', None),\n",
    "        }\n",
    "        for _, row in retrieved.head(top_k).iterrows()\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'retrieved': retrieved,\n",
    "        'refs': refs,\n",
    "    }\n",
    "\n",
    "# пример интерактивного вызова:\n",
    "# debug_question('What is the main idea of the document?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ac49f",
   "metadata": {},
   "source": [
    "## Block 7. Оффлайн-оценка на подвыборке (если есть GT-ответы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_f1(pred: str, gt: str) -> float:\n",
    "    pred_tokens = pred.lower().split()\n",
    "    gt_tokens = gt.lower().split()\n",
    "    if not pred_tokens or not gt_tokens:\n",
    "        return 0.0\n",
    "    common = set(pred_tokens) & set(gt_tokens)\n",
    "    if not common:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gt_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def evaluate_subset(\n",
    "    qa_df: pd.DataFrame,\n",
    "    n: int = 30,\n",
    "    mode: str = 'hybrid',\n",
    ") -> pd.DataFrame:\n",
    "    if QA_GT_ANSWER_COL not in qa_df.columns:\n",
    "        raise ValueError(f'Колонка с GT ответом {QA_GT_ANSWER_COL} не найдена в qa_df')\n",
    "\n",
    "    sub = qa_df.sample(min(n, len(qa_df)), random_state=SEED)\n",
    "    rows = []\n",
    "    for _, row in tqdm(sub.iterrows(), total=len(sub), desc='Eval subset'):\n",
    "        q = str(row[QA_QUESTION_COL])\n",
    "        gt = str(row[QA_GT_ANSWER_COL])\n",
    "\n",
    "        if mode == 'dense':\n",
    "            retrieved = dense_search(q, top_k=TOP_K_FINAL)\n",
    "        elif mode == 'bm25':\n",
    "            retrieved = bm25_search(q, top_k=TOP_K_FINAL)\n",
    "        else:\n",
    "            retrieved = hybrid_search(q, top_k=TOP_K_FINAL)\n",
    "        ctx = retrieved['text'].tolist()\n",
    "        pred = llm_answer(q, ctx)\n",
    "\n",
    "        f1 = simple_f1(pred, gt)\n",
    "        rows.append({\n",
    "            QA_ID_COL: row[QA_ID_COL],\n",
    "            'question': q,\n",
    "            'gt': gt,\n",
    "            'pred': pred,\n",
    "            'f1': f1,\n",
    "        })\n",
    "\n",
    "    res_df = pd.DataFrame(rows)\n",
    "    print('Mean F1:', res_df['f1'].mean())\n",
    "    return res_df\n",
    "\n",
    "# пример вызова:\n",
    "# res_eval = evaluate_subset(qa_df, n=20, mode='hybrid')\n",
    "# res_eval.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d87a4",
   "metadata": {},
   "source": [
    "## Block 8. Быстрое построение submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_submission(\n",
    "    qa_df: pd.DataFrame,\n",
    "    mode: str = 'hybrid',\n",
    "    output_path: str = 'submission.csv',\n",
    ") -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, row in tqdm(qa_df.iterrows(), total=len(qa_df), desc='Build submission'):\n",
    "        q_id = row[QA_ID_COL]\n",
    "        q_text = str(row[QA_QUESTION_COL])\n",
    "\n",
    "        if mode == 'dense':\n",
    "            retrieved = dense_search(q_text, top_k=TOP_K_FINAL)\n",
    "        elif mode == 'bm25':\n",
    "            retrieved = bm25_search(q_text, top_k=TOP_K_FINAL)\n",
    "        else:\n",
    "            retrieved = hybrid_search(q_text, top_k=TOP_K_FINAL)\n",
    "\n",
    "        ctx = retrieved['text'].tolist()\n",
    "        try:\n",
    "            answer = llm_answer(q_text, ctx)\n",
    "        except Exception as e:\n",
    "            answer = f'error: {repr(e)}'\n",
    "\n",
    "        refs = [\n",
    "            {\n",
    "                'doc_id': r['doc_id'],\n",
    "                'chunk_id': r['chunk_id'],\n",
    "                'score_hybrid': r.get('score_hybrid', None),\n",
    "                'score_dense': r.get('score_dense', None),\n",
    "                'score_bm25': r.get('score_bm25', None),\n",
    "            }\n",
    "            for _, r in retrieved.iterrows()\n",
    "        ]\n",
    "\n",
    "        rows.append({\n",
    "            SUBM_ID_COL: q_id,\n",
    "            SUBM_ANSWER_COL: answer,\n",
    "            SUBM_REFS_COL: json.dumps(refs, ensure_ascii=False),\n",
    "        })\n",
    "\n",
    "    subm_df = pd.DataFrame(rows)\n",
    "    subm_df.to_csv(output_path, index=False)\n",
    "    print('Saved submission to:', output_path)\n",
    "    return subm_df\n",
    "\n",
    "# пример:\n",
    "# submission_df = build_submission(qa_df, mode='hybrid', output_path='submission.csv')\n",
    "# submission_df.head()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}