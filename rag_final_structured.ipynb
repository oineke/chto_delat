{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1cbd9ca",
   "metadata": {},
   "source": [
    "# RAG Final Baseline\n",
    "\n",
    "Структурированный ноутбук для финала RAG:\n",
    "- единый конфиг в начале;\n",
    "- загрузка `book.pdf` + `queries.json`;\n",
    "- чанкинг текста;\n",
    "- dense (FAISS) + BM25 + гибридный ретрив;\n",
    "- LLM-ответы и сабмит `submission.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3bbfe8",
   "metadata": {},
   "source": [
    "## Block 0. Конфиг, пути, сиды, девайс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c66d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "BASE_DIR = Path('.').resolve()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "DOCS_DIR = DATA_DIR / 'docs'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DOCS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BOOK_PATH = DOCS_DIR / 'book.pdf'\n",
    "QUERIES_PATH = DATA_DIR / 'queries.json'\n",
    "SUBMISSION_PATH = OUTPUT_DIR / 'submission.csv'\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 128\n",
    "MAX_CONTEXT_CHARS = 4000\n",
    "\n",
    "EMBED_MODEL_ID = os.getenv('RAG_EMBED_MODEL', 'intfloat/multilingual-e5-large').strip()\n",
    "HF_LLM_ID = os.getenv('RAG_LLM_ID', 'Qwen/Qwen2.5-1.5B-Instruct').strip()\n",
    "LOCAL_LLM_PATH = os.getenv('RAG_LLM_PATH', '').strip()  # если модель лежит локально\n",
    "\n",
    "TOP_K_DENSE = int(os.getenv('RAG_TOPK_DENSE', '32'))\n",
    "TOP_K_BM25 = int(os.getenv('RAG_TOPK_SPARSE', '32'))\n",
    "TOP_K_MERGED = int(os.getenv('RAG_TOPK_MERGED', '40'))\n",
    "\n",
    "W_DENSE = float(os.getenv('RAG_W_DENSE', '0.6'))\n",
    "W_SPARSE = float(os.getenv('RAG_W_SPARSE', '0.4'))\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "print('BASE_DIR   :', BASE_DIR)\n",
    "print('DATA_DIR   :', DATA_DIR)\n",
    "print('DOCS_DIR   :', DOCS_DIR)\n",
    "print('OUTPUT_DIR :', OUTPUT_DIR)\n",
    "print('EMBED_MODEL_ID:', EMBED_MODEL_ID)\n",
    "print('HF_LLM_ID      :', HF_LLM_ID)\n",
    "print('LOCAL_LLM_PATH :', LOCAL_LLM_PATH or '(not set)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401c712",
   "metadata": {},
   "source": [
    "## Block 1. Копирование `book.pdf` и `queries.json` в `data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524464ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_book = BASE_DIR / 'book.pdf'\n",
    "dst_book = BOOK_PATH\n",
    "\n",
    "if src_book.exists():\n",
    "    import shutil\n",
    "    shutil.copy2(src_book, dst_book)\n",
    "    print('Скопировал book.pdf →', dst_book)\n",
    "else:\n",
    "    print('НЕ НАЙДЕН book.pdf в корне:', src_book)\n",
    "\n",
    "src_queries = BASE_DIR / 'queries.json'\n",
    "dst_queries = QUERIES_PATH\n",
    "\n",
    "if src_queries.exists():\n",
    "    import shutil\n",
    "    shutil.copy2(src_queries, dst_queries)\n",
    "    print('Скопировал queries.json →', dst_queries)\n",
    "else:\n",
    "    print('НЕ НАЙДЕН queries.json в корне:', src_queries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8aae7b",
   "metadata": {},
   "source": [
    "## Block 2. Документы: поиск и загрузка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from pypdf import PdfReader\n",
    "\n",
    "SUPPORTED_EXTENSIONS = ['.pdf', '.txt', '.md']\n",
    "\n",
    "def list_documents(docs_dir: Path, exts=None) -> pd.DataFrame:\n",
    "    if exts is None:\n",
    "        exts = SUPPORTED_EXTENSIONS\n",
    "    docs = []\n",
    "    if not docs_dir.exists():\n",
    "        docs_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return pd.DataFrame(columns=['doc_id', 'path', 'ext'])\n",
    "    for path in sorted(docs_dir.rglob('*')):\n",
    "        if path.is_file():\n",
    "            ext = path.suffix.lower()\n",
    "            if ext in exts:\n",
    "                docs.append({'doc_id': len(docs), 'path': path, 'ext': ext})\n",
    "    return pd.DataFrame(docs)\n",
    "\n",
    "def load_pdf(path: Path) -> str:\n",
    "    reader = PdfReader(str(path))\n",
    "    parts = []\n",
    "    for page in reader.pages:\n",
    "        try:\n",
    "            txt = page.extract_text() or ''\n",
    "        except Exception:\n",
    "            txt = ''\n",
    "        parts.append(txt)\n",
    "    return '\\n\\n'.join(parts)\n",
    "\n",
    "def load_text_file(path: Path, encoding: str = 'utf-8') -> str:\n",
    "    with open(path, 'r', encoding=encoding, errors='ignore') as f:\n",
    "        return f.read()\n",
    "\n",
    "def load_single_document(doc_row: pd.Series) -> Dict[str, Any]:\n",
    "    doc_id = int(doc_row['doc_id'])\n",
    "    path = Path(doc_row['path'])\n",
    "    ext = str(doc_row['ext']).lower()\n",
    "    if ext == '.pdf':\n",
    "        text = load_pdf(path)\n",
    "    else:\n",
    "        text = load_text_file(path)\n",
    "    return {\n",
    "        'doc_id': doc_id,\n",
    "        'path': str(path),\n",
    "        'ext': ext,\n",
    "        'text': text,\n",
    "        'n_chars': len(text),\n",
    "    }\n",
    "\n",
    "docs_df = list_documents(DOCS_DIR)\n",
    "print('Найдено документов:', len(docs_df))\n",
    "print(docs_df.head())\n",
    "\n",
    "raw_docs: List[Dict[str, Any]] = []\n",
    "if len(docs_df) == 0:\n",
    "    print('Нет документов в DOCS_DIR, дальше RAG не заработает.')\n",
    "else:\n",
    "    for _, row in tqdm(docs_df.iterrows(), total=len(docs_df), desc='Загрузка документов'):\n",
    "        raw_docs.append(load_single_document(row))\n",
    "\n",
    "raw_docs_df = pd.DataFrame(raw_docs)\n",
    "print('raw_docs_df shape:', raw_docs_df.shape)\n",
    "print(raw_docs_df[['doc_id', 'ext', 'n_chars']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4be9a5",
   "metadata": {},
   "source": [
    "## Block 3. Чанкинг текста документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcde39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def split_text_to_chunks(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    chunks = []\n",
    "    if not text:\n",
    "        return chunks\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "    while i < n:\n",
    "        chunk = text[i : i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "        i += step\n",
    "    return chunks\n",
    "\n",
    "all_chunks: List[Dict[str, Any]] = []\n",
    "\n",
    "if raw_docs_df is None or len(raw_docs_df) == 0:\n",
    "    print('raw_docs_df пуст — сначала нужно загрузить документы (Block 2).')\n",
    "else:\n",
    "    for _, row in tqdm(raw_docs_df.iterrows(), total=len(raw_docs_df), desc='Chunking'):\n",
    "        doc_id = int(row['doc_id'])\n",
    "        text = str(row['text'])\n",
    "        path = str(row['path'])\n",
    "        ext = str(row['ext'])\n",
    "        chunks = split_text_to_chunks(text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "        for local_idx, ch in enumerate(chunks):\n",
    "            all_chunks.append(\n",
    "                {\n",
    "                    'chunk_id': None,\n",
    "                    'doc_id': doc_id,\n",
    "                    'chunk_idx_in_doc': local_idx,\n",
    "                    'path': path,\n",
    "                    'ext': ext,\n",
    "                    'text': ch,\n",
    "                    'n_chars': len(ch),\n",
    "                }\n",
    "            )\n",
    "\n",
    "for idx, ch in enumerate(all_chunks):\n",
    "    ch['chunk_id'] = idx\n",
    "\n",
    "chunks_df = pd.DataFrame(all_chunks)\n",
    "print('chunks_df shape:', chunks_df.shape)\n",
    "print(chunks_df[['chunk_id', 'doc_id', 'chunk_idx_in_doc', 'n_chars']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa6f3d2",
   "metadata": {},
   "source": [
    "## Block 4. Эмбеддинги чанков и FAISS-индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "print('Загружаю embedding-модель:', EMBED_MODEL_ID)\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_ID, device=str(DEVICE))\n",
    "\n",
    "test_emb = embed_model.encode(['test'], convert_to_numpy=True, show_progress_bar=False)\n",
    "EMBED_DIM = int(test_emb.shape[1])\n",
    "print('EMBED_DIM:', EMBED_DIM)\n",
    "\n",
    "def embed_texts(texts, batch_size: int = 64):\n",
    "    emb = embed_model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    return emb.astype('float32')\n",
    "\n",
    "if chunks_df is None or len(chunks_df) == 0:\n",
    "    print('chunks_df пуст — сначала нужно сделать чанкинг.')\n",
    "else:\n",
    "    texts = chunks_df['text'].tolist()\n",
    "    batch_size = 256\n",
    "    embs_list = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc='Embedding chunks'):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        embs = embed_texts(batch)\n",
    "        embs_list.append(embs)\n",
    "    embeddings_np = np.vstack(embs_list).astype('float32')\n",
    "    print('embeddings_np shape:', embeddings_np.shape)\n",
    "\n",
    "    index = faiss.IndexFlatIP(EMBED_DIM)\n",
    "    index.add(embeddings_np)\n",
    "    faiss_index = index\n",
    "    print('FAISS index ready, n_vectors =', faiss_index.ntotal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b830a9d",
   "metadata": {},
   "source": [
    "## Block 5. BM25 по чанкам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def bm25_tokenize(text: str):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r\"\\w+\", text, flags=re.UNICODE)\n",
    "    return tokens\n",
    "\n",
    "bm25_corpus_tokens = [bm25_tokenize(t) for t in chunks_df['text'].tolist()]\n",
    "bm25_chunk_ids = chunks_df['chunk_id'].tolist()\n",
    "bm25 = BM25Okapi(bm25_corpus_tokens)\n",
    "print('BM25 corpus size:', len(bm25_corpus_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4cf642",
   "metadata": {},
   "source": [
    "## Block 6. Ретраивер: dense, BM25, hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcab517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _minmax_normalize(arr: np.ndarray) -> np.ndarray:\n",
    "    if arr is None or len(arr) == 0:\n",
    "        return np.zeros_like(arr)\n",
    "    a_min = float(np.min(arr))\n",
    "    a_max = float(np.max(arr))\n",
    "    if not np.isfinite(a_min) or not np.isfinite(a_max) or a_max <= a_min:\n",
    "        return np.zeros_like(arr)\n",
    "    return (arr - a_min) / (a_max - a_min)\n",
    "\n",
    "def dense_search(query: str):\n",
    "    if 'faiss_index' not in globals() or faiss_index is None:\n",
    "        return []\n",
    "    q_emb = embed_texts([query], batch_size=1)\n",
    "    D, I = faiss_index.search(q_emb, TOP_K_DENSE)\n",
    "    scores = D[0]\n",
    "    ids = I[0]\n",
    "    scores_norm = _minmax_normalize(scores)\n",
    "    out = []\n",
    "    for cid, s, sn in zip(ids, scores, scores_norm):\n",
    "        cid = int(cid)\n",
    "        if cid < 0:\n",
    "            continue\n",
    "        out.append((cid, float(s), float(sn)))\n",
    "    return out\n",
    "\n",
    "def sparse_search(query: str):\n",
    "    if 'bm25' not in globals():\n",
    "        return []\n",
    "    tokens = bm25_tokenize(query)\n",
    "    scores = bm25.get_scores(tokens)\n",
    "    if scores.size == 0:\n",
    "        return []\n",
    "    top_k = min(TOP_K_BM25, scores.shape[0])\n",
    "    top_idx = np.argsort(scores)[-top_k:][::-1]\n",
    "    top_scores = scores[top_idx]\n",
    "    scores_norm = _minmax_normalize(top_scores)\n",
    "    out = []\n",
    "    for i, s, sn in zip(top_idx, top_scores, scores_norm):\n",
    "        cid = int(bm25_chunk_ids[int(i)])\n",
    "        out.append((cid, float(s), float(sn)))\n",
    "    return out\n",
    "\n",
    "def hybrid_search_one(qid: int, query: str):\n",
    "    dense = dense_search(query)\n",
    "    sparse = sparse_search(query)\n",
    "    dense_dict = {cid: (s_raw, s_norm) for cid, s_raw, s_norm in dense}\n",
    "    sparse_dict = {cid: (s_raw, s_norm) for cid, s_norm in sparse}\n",
    "    candidates = set(dense_dict.keys()) | set(sparse_dict.keys())\n",
    "    rows = []\n",
    "    for cid in candidates:\n",
    "        d_raw, d_norm = dense_dict.get(cid, (0.0, 0.0))\n",
    "        s_raw, s_norm = sparse_dict.get(cid, (0.0, 0.0))\n",
    "        hybrid = W_DENSE * d_norm + W_SPARSE * s_norm\n",
    "        rows.append({\n",
    "            'qid': int(qid),\n",
    "            'query': str(query),\n",
    "            'chunk_id': int(cid),\n",
    "            'dense_score': float(d_raw),\n",
    "            'dense_norm': float(d_norm),\n",
    "            'bm25_score': float(s_raw),\n",
    "            'bm25_norm': float(s_norm),\n",
    "            'hybrid_score': float(hybrid),\n",
    "        })\n",
    "    rows_sorted = sorted(rows, key=lambda r: r['hybrid_score'], reverse=True)[:TOP_K_MERGED]\n",
    "    return rows_sorted\n",
    "\n",
    "def hybrid_search_batch(queries, query_ids=None) -> pd.DataFrame:\n",
    "    if query_ids is None:\n",
    "        query_ids = list(range(len(queries)))\n",
    "    rows = []\n",
    "    for qid, q in tqdm(list(zip(query_ids, queries)), total=len(queries), desc='Hybrid search'):\n",
    "        rows.extend(hybrid_search_one(qid, q))\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dffed3",
   "metadata": {},
   "source": [
    "## Block 7. Загрузка LLM (Qwen или другая HF-модель)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "if LOCAL_LLM_PATH:\n",
    "    model_path = LOCAL_LLM_PATH\n",
    "else:\n",
    "    model_path = HF_LLM_ID\n",
    "\n",
    "print('Загружаю LLM из:', model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "gen_pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if DEVICE.type == 'cuda' else -1,\n",
    "    torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n",
    ")\n",
    "\n",
    "print('LLM готов.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34fea2",
   "metadata": {},
   "source": [
    "## Block 8. RAG: retrieve_relevant_chunks + генерация ответа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf70e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    'You are an expert in document analysis and retrieval-augmented generation (RAG). '\n",
    "    'You receive a CONTEXT (fragments from documents) and a QUESTION. '\n",
    "    'Answer strictly based on the context. '\n",
    "    'If the context is not sufficient, say that you do not know and avoid hallucinations.'\n",
    ")\n",
    "\n",
    "def build_prompt(query: str, context_text: str) -> str:\n",
    "    return (\n",
    "        SYSTEM_PROMPT\n",
    "        + '\\n\\nContext:\\n'\n",
    "        + context_text\n",
    "        + '\\n\\nQuestion:\\n'\n",
    "        + query\n",
    "        + '\\n\\nAnswer:'\n",
    "    )\n",
    "\n",
    "def generate_answer(query: str, context_text: str, max_new_tokens: int = 256, temperature: float = 0.2, top_p: float = 0.9) -> str:\n",
    "    prompt = build_prompt(query, context_text)\n",
    "    out = gen_pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    text = out[0]['generated_text']\n",
    "    if 'Answer:' in text:\n",
    "        text = text.split('Answer:', 1)[1].strip()\n",
    "    return text.strip()\n",
    "\n",
    "def retrieve_relevant_chunks(query: str, top_k: int = 8):\n",
    "    rows = hybrid_search_one(0, query)\n",
    "    df = pd.DataFrame(rows)\n",
    "    if len(df) == 0:\n",
    "        return {'context_text': '', 'chunks': []}\n",
    "    merged = df.merge(chunks_df, on='chunk_id', how='left')\n",
    "    merged = merged.sort_values('hybrid_score', ascending=False).head(top_k)\n",
    "    texts = merged['text'].tolist()\n",
    "    context_text = '\\n\\n---\\n\\n'.join(texts)\n",
    "    if len(context_text) > MAX_CONTEXT_CHARS:\n",
    "        context_text = context_text[:MAX_CONTEXT_CHARS]\n",
    "    chunk_infos = []\n",
    "    for _, r in merged.iterrows():\n",
    "        chunk_infos.append(\n",
    "            {\n",
    "                'chunk_id': int(r['chunk_id']),\n",
    "                'doc_id': int(r['doc_id']),\n",
    "                'score': float(r['hybrid_score']),\n",
    "                'path': r['path'],\n",
    "            }\n",
    "        )\n",
    "    return {'context_text': context_text, 'chunks': chunk_infos}\n",
    "\n",
    "def rag_answer_with_context_and_refs(query: str, max_new_tokens: int = 256, temperature: float = 0.2, top_p: float = 0.9):\n",
    "    retrieval = retrieve_relevant_chunks(query)\n",
    "    context_text = retrieval['context_text']\n",
    "    chunks = retrieval['chunks']\n",
    "    if not context_text.strip():\n",
    "        answer = 'I could not find enough relevant information in the documents to answer this question.'\n",
    "        refs_dict = {'chunks': chunks}\n",
    "        refs_json = json.dumps(refs_dict, ensure_ascii=False)\n",
    "        return answer, context_text, refs_json, refs_dict\n",
    "    answer = generate_answer(query, context_text, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p)\n",
    "    refs_dict = {'chunks': chunks}\n",
    "    refs_json = json.dumps(refs_dict, ensure_ascii=False)\n",
    "    return answer, context_text, refs_json, refs_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4543a88",
   "metadata": {},
   "source": [
    "## Block 9. Загрузка вопросов и формирование submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c703a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not QUERIES_PATH.exists():\n",
    "    raise FileNotFoundError(f'Не найден файл с вопросами: {QUERIES_PATH}')\n",
    "\n",
    "with open(QUERIES_PATH, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "qa_df = pd.DataFrame(data)\n",
    "print('qa_df columns:', list(qa_df.columns))\n",
    "print(qa_df.head())\n",
    "\n",
    "if 'question' not in qa_df.columns and 'query' in qa_df.columns:\n",
    "    qa_df.rename(columns={'query': 'question'}, inplace=True)\n",
    "\n",
    "if 'id' not in qa_df.columns:\n",
    "    raise KeyError('В queries.json должна быть колонка id.')\n",
    "\n",
    "rows = []\n",
    "for _, row in tqdm(qa_df.iterrows(), total=len(qa_df), desc='RAG answers'):\n",
    "    q_id = row['id']\n",
    "    q_text = str(row['question'])\n",
    "    try:\n",
    "        answer, context_text, refs_json, refs_dict = rag_answer_with_context_and_refs(\n",
    "            q_text,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        answer = f'error: {repr(e)}'\n",
    "        refs_dict = {'error': repr(e)}\n",
    "        refs_json = json.dumps(refs_dict, ensure_ascii=False)\n",
    "    rows.append({'id': q_id, 'answer': answer, 'refs_json': refs_json})\n",
    "\n",
    "submission_df = pd.DataFrame(rows)\n",
    "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "print('Saved submission to:', SUBMISSION_PATH)\n",
    "submission_df.head()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}