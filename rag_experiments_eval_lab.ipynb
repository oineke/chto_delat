{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c7f02d",
   "metadata": {},
   "source": [
    "# RAG Experiments & Evaluation Lab\n",
    "\n",
    "Ноутбук для **экспериментов и оффлайн-оценки RAG**:\n",
    "\n",
    "- единый конфиг для данных и моделей;\n",
    "- dense / BM25 / hybrid ретриверы;\n",
    "- запуск нескольких конфигураций (экспериментов) подряд;\n",
    "- метрики по ответам (простой F1) и по ретриву (recall@k по целевому doc_id, если он есть);\n",
    "- таблица результатов, готовая для сохранения и заливки на GitHub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5bec37",
   "metadata": {},
   "source": [
    "## Block 0. Setup & конфиг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8058466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers rank-bm25 datasets nltk --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "\n",
    "# === Конфиг данных ===\n",
    "DATA_DIR = Path('/kaggle/input')  # поменяй под себя\n",
    "DOC_SOURCE = 'hf'  # 'hf' или 'local'\n",
    "\n",
    "# HF dataset (тексты + QA)\n",
    "HF_DATASET_NAME = 'your/dataset'   # замени\n",
    "HF_SPLIT_DOCS = 'train'\n",
    "HF_SPLIT_QA = 'validation'\n",
    "HF_DOC_TEXT_COL = 'content'\n",
    "HF_DOC_ID_COL = 'id'  # если другая колонка id — поменяй\n",
    "\n",
    "# Локальный вариант (если DOC_SOURCE='local')\n",
    "LOCAL_DOCS_CSV = DATA_DIR / 'docs.csv'  # docs.csv: doc_id,text\n",
    "LOCAL_QA_CSV = DATA_DIR / 'qa.csv'      # qa.csv: id,question,answer[,doc_id]\n",
    "LOCAL_DOC_ID_COL = 'doc_id'\n",
    "LOCAL_DOC_TEXT_COL = 'text'\n",
    "\n",
    "# === Chunking ===\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 200\n",
    "MIN_CHARS = 50\n",
    "\n",
    "# === Эмбеддер + LLM ===\n",
    "EMBED_MODEL_ID = 'intfloat/multilingual-e5-large'\n",
    "GEN_MODEL_PATH = '/kaggle/input/qwen2.5/transformers/1.5b-instruct/1'  # замени при необходимости\n",
    "MAX_NEW_TOKENS = 256\n",
    "TEMP = 0.2\n",
    "TOP_P = 0.9\n",
    "\n",
    "# === Колонки QA/submit ===\n",
    "QA_ID_COL = 'id'\n",
    "QA_QUESTION_COL = 'question'\n",
    "QA_GT_ANSWER_COL = 'answer'  # если нет GT — метрика по ответам работать не будет\n",
    "QA_GT_DOCID_COL = 'doc_id'   # опционально: id целевого документа, если есть\n",
    "\n",
    "SUBM_ID_COL = 'id'\n",
    "SUBM_ANSWER_COL = 'answer'\n",
    "SUBM_REFS_COL = 'refs_json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a72525",
   "metadata": {},
   "source": [
    "## Block 1. Загрузка данных (docs + QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac5c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs_and_qa() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if DOC_SOURCE == 'hf':\n",
    "        ds_docs = load_dataset(HF_DATASET_NAME, split=HF_SPLIT_DOCS)\n",
    "        docs_df = ds_docs.to_pandas()\n",
    "        if HF_DOC_ID_COL not in docs_df.columns:\n",
    "            docs_df[HF_DOC_ID_COL] = np.arange(len(docs_df))\n",
    "        ds_qa = load_dataset(HF_DATASET_NAME, split=HF_SPLIT_QA)\n",
    "        qa_df = ds_qa.to_pandas()\n",
    "    else:\n",
    "        docs_df = pd.read_csv(LOCAL_DOCS_CSV)\n",
    "        qa_df = pd.read_csv(LOCAL_QA_CSV)\n",
    "\n",
    "    print('Docs shape:', docs_df.shape)\n",
    "    print('QA shape:', qa_df.shape)\n",
    "    return docs_df, qa_df\n",
    "\n",
    "docs_df, qa_df = load_docs_and_qa()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768f18c7",
   "metadata": {},
   "source": [
    "## Block 2. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87624715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def make_chunks_from_docs(\n",
    "    docs_df: pd.DataFrame,\n",
    "    text_col: str,\n",
    "    id_col: str,\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "    chunk_overlap: int = CHUNK_OVERLAP,\n",
    "    min_chars: int = MIN_CHARS,\n",
    ") -> pd.DataFrame:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=['\\n\\n', '\\n', '. ', ' ', ''],\n",
    "    )\n",
    "    rows = []\n",
    "    for _, row in tqdm(docs_df.iterrows(), total=len(docs_df), desc='Chunking docs'):\n",
    "        doc_id = row[id_col]\n",
    "        text = str(row[text_col])\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        parts = splitter.split_text(text)\n",
    "        for idx, ch in enumerate(parts):\n",
    "            ch = ch.strip()\n",
    "            if len(ch) < min_chars:\n",
    "                continue\n",
    "            rows.append({'doc_id': doc_id, 'chunk_id': f'{doc_id}_{idx}', 'text': ch})\n",
    "    chunks_df = pd.DataFrame(rows)\n",
    "    print('Total chunks:', chunks_df.shape)\n",
    "    return chunks_df\n",
    "\n",
    "if DOC_SOURCE == 'hf':\n",
    "    chunks_df = make_chunks_from_docs(docs_df, text_col=HF_DOC_TEXT_COL, id_col=HF_DOC_ID_COL)\n",
    "else:\n",
    "    chunks_df = make_chunks_from_docs(docs_df, text_col=LOCAL_DOC_TEXT_COL, id_col=LOCAL_DOC_ID_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f1b585",
   "metadata": {},
   "source": [
    "## Block 3. Эмбеддинги + in-memory индекс + BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09eedf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(EMBED_MODEL_ID, device=str(device))\n",
    "embedding_model.eval()\n",
    "print('Loaded embedder:', EMBED_MODEL_ID)\n",
    "\n",
    "def build_dense_index(chunks_df: pd.DataFrame) -> np.ndarray:\n",
    "    texts = chunks_df['text'].tolist()\n",
    "    vecs = embedding_model.encode(\n",
    "        texts,\n",
    "        batch_size=64,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    return np.array(vecs, dtype=np.float32)\n",
    "\n",
    "dense_matrix = build_dense_index(chunks_df)\n",
    "print('dense_matrix shape:', dense_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c4c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_chunks = [word_tokenize(t) for t in chunks_df['text'].tolist()]\n",
    "bm25 = BM25Okapi(tokenized_chunks)\n",
    "print('BM25 corpus size:', len(tokenized_chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3b80cd",
   "metadata": {},
   "source": [
    "## Block 4. Ретраиверы (dense / BM25 / hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_search(query: str, top_k: int) -> pd.DataFrame:\n",
    "    q_vec = embedding_model.encode([query], normalize_embeddings=True)\n",
    "    q_vec = q_vec.astype(np.float32)[0]\n",
    "    sims = dense_matrix @ q_vec\n",
    "    idx = np.argsort(-sims)[:top_k]\n",
    "    sub = chunks_df.iloc[idx].copy()\n",
    "    sub['score_dense'] = sims[idx]\n",
    "    return sub\n",
    "\n",
    "def bm25_search(query: str, top_k: int) -> pd.DataFrame:\n",
    "    toks = word_tokenize(query)\n",
    "    scores = bm25.get_scores(toks)\n",
    "    idx = np.argsort(-scores)[:top_k]\n",
    "    sub = chunks_df.iloc[idx].copy()\n",
    "    sub['score_bm25'] = scores[idx]\n",
    "    return sub\n",
    "\n",
    "def hybrid_search(query: str, top_k_dense: int, top_k_bm25: int, alpha: float, top_k_final: int) -> pd.DataFrame:\n",
    "    d = dense_search(query, top_k=top_k_dense)\n",
    "    b = bm25_search(query, top_k=top_k_bm25)\n",
    "\n",
    "    d_ = d[['doc_id', 'chunk_id', 'score_dense']]\n",
    "    b_ = b[['doc_id', 'chunk_id', 'score_bm25']]\n",
    "    merged = pd.merge(d_, b_, on=['doc_id', 'chunk_id'], how='outer')\n",
    "\n",
    "    max_d = merged['score_dense'].max() if merged['score_dense'].notna().any() else 1.0\n",
    "    max_b = merged['score_bm25'].max() if merged['score_bm25'].notna().any() else 1.0\n",
    "    merged['score_dense_n'] = merged['score_dense'] / max_d\n",
    "    merged['score_bm25_n'] = merged['score_bm25'] / max_b\n",
    "    merged['score_hybrid'] = alpha * merged['score_dense_n'].fillna(0) + (1 - alpha) * merged['score_bm25_n'].fillna(0)\n",
    "\n",
    "    out = pd.merge(merged, chunks_df, on=['doc_id', 'chunk_id'], how='left')\n",
    "    out = out.sort_values('score_hybrid', ascending=False).head(top_k_final).reset_index(drop=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ac5e5",
   "metadata": {},
   "source": [
    "## Block 5. LLM-генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e82e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pipeline = pipeline(\n",
    "    'text-generation',\n",
    "    model=GEN_MODEL_PATH,\n",
    "    device=0 if device.type == 'cuda' else -1,\n",
    "    torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
    ")\n",
    "print('Loaded generation model from:', GEN_MODEL_PATH)\n",
    "\n",
    "SYSTEM_PROMPT = '''You are a helpful assistant that answers questions using the provided context.\n",
    "Answer in the same language as the question.\n",
    "If the answer cannot be found in the context, say that you don't know and avoid hallucinations.\n",
    "'''\n",
    "\n",
    "def build_prompt(query: str, context_chunks: List[str]) -> str:\n",
    "    ctx = '\\n\\n'.join(context_chunks)\n",
    "    return f\"{SYSTEM_PROMPT}\\n\\nContext:\\n{ctx}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\"\n",
    "\n",
    "def llm_answer(query: str, context_chunks: List[str]) -> str:\n",
    "    prompt = build_prompt(query, context_chunks)\n",
    "    out = generation_pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=TEMP,\n",
    "        top_p=TOP_P,\n",
    "        pad_token_id=generation_pipeline.tokenizer.eos_token_id,\n",
    "    )\n",
    "    text = out[0]['generated_text']\n",
    "    if 'Answer:' in text:\n",
    "        text = text.split('Answer:', 1)[1].strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab944cd",
   "metadata": {},
   "source": [
    "## Block 6. Метрики (по ответам и по ретриву)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08dff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_f1(pred: str, gt: str) -> float:\n",
    "    pred_tokens = pred.lower().split()\n",
    "    gt_tokens = gt.lower().split()\n",
    "    if not pred_tokens or not gt_tokens:\n",
    "        return 0.0\n",
    "    common = set(pred_tokens) & set(gt_tokens)\n",
    "    if not common:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gt_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def recall_at_k_doc(gt_doc_id, retrieved_df: pd.DataFrame) -> float:\n",
    "    if gt_doc_id is None:\n",
    "        return np.nan\n",
    "    if 'doc_id' not in retrieved_df.columns:\n",
    "        return np.nan\n",
    "    return float(gt_doc_id in retrieved_df['doc_id'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112dc4fc",
   "metadata": {},
   "source": [
    "## Block 7. Один эксперимент: конфиг → метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(\n",
    "    qa_df: pd.DataFrame,\n",
    "    name: str,\n",
    "    mode: str = 'hybrid',  # 'dense', 'bm25', 'hybrid'\n",
    "    top_k_dense: int = 15,\n",
    "    top_k_bm25: int = 15,\n",
    "    alpha: float = 0.5,\n",
    "    top_k_final: int = 8,\n",
    "    n_samples: int = 50,\n",
    ") -> Dict[str, Any]:\n",
    "    if n_samples is not None and n_samples < len(qa_df):\n",
    "        data = qa_df.sample(n_samples, random_state=SEED)\n",
    "    else:\n",
    "        data = qa_df\n",
    "\n",
    "    has_gt_answer = QA_GT_ANSWER_COL in data.columns\n",
    "    has_gt_doc = QA_GT_DOCID_COL in data.columns\n",
    "\n",
    "    f1_scores = []\n",
    "    r_doc_scores = []\n",
    "\n",
    "    for _, row in tqdm(data.iterrows(), total=len(data), desc=f'Experiment: {name}'):\n",
    "        q = str(row[QA_QUESTION_COL])\n",
    "\n",
    "        if mode == 'dense':\n",
    "            retrieved = dense_search(q, top_k=top_k_final)\n",
    "        elif mode == 'bm25':\n",
    "            retrieved = bm25_search(q, top_k=top_k_final)\n",
    "        else:\n",
    "            retrieved = hybrid_search(q, top_k_dense=top_k_dense, top_k_bm25=top_k_bm25, alpha=alpha, top_k_final=top_k_final)\n",
    "\n",
    "        ctx = retrieved['text'].tolist()\n",
    "        pred = llm_answer(q, ctx)\n",
    "\n",
    "        if has_gt_answer:\n",
    "            gt = str(row[QA_GT_ANSWER_COL])\n",
    "            f1_scores.append(simple_f1(pred, gt))\n",
    "\n",
    "        if has_gt_doc:\n",
    "            gt_doc = row[QA_GT_DOCID_COL]\n",
    "            r_doc_scores.append(recall_at_k_doc(gt_doc, retrieved))\n",
    "\n",
    "    res = {\n",
    "        'name': name,\n",
    "        'mode': mode,\n",
    "        'top_k_dense': top_k_dense,\n",
    "        'top_k_bm25': top_k_bm25,\n",
    "        'alpha': alpha,\n",
    "        'top_k_final': top_k_final,\n",
    "        'n_samples': len(data),\n",
    "    }\n",
    "\n",
    "    if f1_scores:\n",
    "        res['mean_f1'] = float(np.mean(f1_scores))\n",
    "    else:\n",
    "        res['mean_f1'] = np.nan\n",
    "\n",
    "    if r_doc_scores:\n",
    "        res['mean_recall_doc'] = float(np.nanmean(r_doc_scores))\n",
    "    else:\n",
    "        res['mean_recall_doc'] = np.nan\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e2ffc",
   "metadata": {},
   "source": [
    "## Block 8. Набор экспериментов и сводная таблица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS = [\n",
    "    {\n",
    "        'name': 'dense_k8',\n",
    "        'mode': 'dense',\n",
    "        'top_k_dense': 8,\n",
    "        'top_k_bm25': 0,\n",
    "        'alpha': 0.5,\n",
    "        'top_k_final': 8,\n",
    "        'n_samples': 40,\n",
    "    },\n",
    "    {\n",
    "        'name': 'bm25_k8',\n",
    "        'mode': 'bm25',\n",
    "        'top_k_dense': 0,\n",
    "        'top_k_bm25': 8,\n",
    "        'alpha': 0.5,\n",
    "        'top_k_final': 8,\n",
    "        'n_samples': 40,\n",
    "    },\n",
    "    {\n",
    "        'name': 'hybrid_a0.5_k8',\n",
    "        'mode': 'hybrid',\n",
    "        'top_k_dense': 15,\n",
    "        'top_k_bm25': 15,\n",
    "        'alpha': 0.5,\n",
    "        'top_k_final': 8,\n",
    "        'n_samples': 40,\n",
    "    },\n",
    "    {\n",
    "        'name': 'hybrid_a0.7_k10',\n",
    "        'mode': 'hybrid',\n",
    "        'top_k_dense': 20,\n",
    "        'top_k_bm25': 10,\n",
    "        'alpha': 0.7,\n",
    "        'top_k_final': 10,\n",
    "        'n_samples': 40,\n",
    "    },\n",
    "]\n",
    "\n",
    "results = []\n",
    "for cfg in EXPERIMENTS:\n",
    "    res = run_single_experiment(\n",
    "        qa_df=qa_df,\n",
    "        name=cfg['name'],\n",
    "        mode=cfg['mode'],\n",
    "        top_k_dense=cfg['top_k_dense'],\n",
    "        top_k_bm25=cfg['top_k_bm25'],\n",
    "        alpha=cfg['alpha'],\n",
    "        top_k_final=cfg['top_k_final'],\n",
    "        n_samples=cfg['n_samples'],\n",
    "    )\n",
    "    results.append(res)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(['mean_f1', 'mean_recall_doc'], ascending=[False, False])\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dcda53",
   "metadata": {},
   "source": [
    "## Block 9. Сохранить результаты и helper для submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('rag_experiments_results.csv', index=False)\n",
    "print('Saved rag_experiments_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_submission(\n",
    "    qa_df: pd.DataFrame,\n",
    "    mode: str = 'hybrid',\n",
    "    top_k_dense: int = 15,\n",
    "    top_k_bm25: int = 15,\n",
    "    alpha: float = 0.5,\n",
    "    top_k_final: int = 8,\n",
    "    output_path: str = 'submission.csv',\n",
    ") -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, row in tqdm(qa_df.iterrows(), total=len(qa_df), desc='Build submission'):\n",
    "        q_id = row[QA_ID_COL]\n",
    "        q_text = str(row[QA_QUESTION_COL])\n",
    "\n",
    "        if mode == 'dense':\n",
    "            retrieved = dense_search(q_text, top_k=top_k_final)\n",
    "        elif mode == 'bm25':\n",
    "            retrieved = bm25_search(q_text, top_k=top_k_final)\n",
    "        else:\n",
    "            retrieved = hybrid_search(q_text, top_k_dense=top_k_dense, top_k_bm25=top_k_bm25, alpha=alpha, top_k_final=top_k_final)\n",
    "\n",
    "        ctx = retrieved['text'].tolist()\n",
    "        try:\n",
    "            answer = llm_answer(q_text, ctx)\n",
    "        except Exception as e:\n",
    "            answer = f'error: {repr(e)}'\n",
    "\n",
    "        refs = [\n",
    "            {\n",
    "                'doc_id': r['doc_id'],\n",
    "                'chunk_id': r['chunk_id'],\n",
    "                'score_dense': r.get('score_dense', None),\n",
    "                'score_bm25': r.get('score_bm25', None),\n",
    "                'score_hybrid': r.get('score_hybrid', None),\n",
    "            }\n",
    "            for _, r in retrieved.iterrows()\n",
    "        ]\n",
    "\n",
    "        rows.append({\n",
    "            SUBM_ID_COL: q_id,\n",
    "            SUBM_ANSWER_COL: answer,\n",
    "            SUBM_REFS_COL: json.dumps(refs, ensure_ascii=False),\n",
    "        })\n",
    "\n",
    "    subm_df = pd.DataFrame(rows)\n",
    "    subm_df.to_csv(output_path, index=False)\n",
    "    print('Saved submission to:', output_path)\n",
    "    return subm_df\n",
    "\n",
    "# пример: после выбора лучшего конфига\n",
    "# submission_df = build_submission(qa_df, mode='hybrid', top_k_dense=15, top_k_bm25=15, alpha=0.5, top_k_final=8, output_path='submission.csv')\n",
    "# submission_df.head()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}