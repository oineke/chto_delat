{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f77b7352",
   "metadata": {},
   "source": [
    "# RAG Submissions Blender\n",
    "\n",
    "Ноутбук для **ансамблирования (blending)** сабмитов по RAG.\n",
    "\n",
    "Цель: из двух (или более) сабмитов получить один итоговый `submission.csv`,\n",
    "используя простые, прозрачные правила без обучения нейросети.\n",
    "\n",
    "Основной кейс — у тебя есть два файла:\n",
    "\n",
    "- `sub1.csv` — сабмит №1 (например, baseline);\n",
    "- `sub2.csv` — сабмит №2 (например, более тяжёлый или с другой моделью),\n",
    "\n",
    "и нужно для каждого `id` выбрать лучший ответ или аккуратно их комбинировать."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e235010",
   "metadata": {},
   "source": [
    "## Block 0. Конфиг и импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc23629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import difflib\n",
    "import re\n",
    "\n",
    "BASE_DIR = Path('.').resolve()\n",
    "INPUT_DIR = BASE_DIR / 'inputs'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "\n",
    "INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Пути к сабмитам (можно поменять под свою среду)\n",
    "SUB1_PATH = INPUT_DIR / 'submission_1.csv'\n",
    "SUB2_PATH = INPUT_DIR / 'submission_2.csv'\n",
    "\n",
    "# (опционально) файл с ground truth для оффлайн-оценки\n",
    "GT_PATH = INPUT_DIR / 'ground_truth.csv'  # можно оставить несуществующим\n",
    "\n",
    "# Конфигурация колонок\n",
    "ID_COL = 'id'\n",
    "ANSWER_COL = 'answer'\n",
    "REFS_COL = 'refs_json'  # если в сабмитах есть колонка с ссылками/чанками\n",
    "\n",
    "# Пороговые значения для правил blending\n",
    "MIN_ANSWER_LEN = 15   # ответы короче считаем подозрительно короткими\n",
    "MAX_ANSWER_LEN = 800  # ответы длиннее считаем \"простынями\"\n",
    "VERY_SIMILAR_THRESHOLD = 0.9  # похожесть текстов (SequenceMatcher)\n",
    "\n",
    "print('BASE_DIR  :', BASE_DIR)\n",
    "print('INPUT_DIR :', INPUT_DIR)\n",
    "print('OUTPUT_DIR:', OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05f96d",
   "metadata": {},
   "source": [
    "## Block 1. Загрузка и выравнивание сабмитов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b328dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_submission(path: Path, name: str) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Не найден сабмит {name}: {path}')\n",
    "    df = pd.read_csv(path)\n",
    "    print(f'{name}: shape =', df.shape)\n",
    "    if ID_COL not in df.columns or ANSWER_COL not in df.columns:\n",
    "        raise KeyError(f'В сабмите {name} должны быть колонки {ID_COL!r} и {ANSWER_COL!r}')\n",
    "    return df\n",
    "\n",
    "sub1 = load_submission(SUB1_PATH, 'sub1')\n",
    "sub2 = load_submission(SUB2_PATH, 'sub2')\n",
    "\n",
    "# sanity-check по id\n",
    "ids1 = set(sub1[ID_COL])\n",
    "ids2 = set(sub2[ID_COL])\n",
    "print('\\nУникальных id в sub1:', len(ids1))\n",
    "print('Уникальных id в sub2:', len(ids2))\n",
    "print('Пересечение id:', len(ids1 & ids2))\n",
    "\n",
    "if ids1 != ids2:\n",
    "    print('\\nWARNING: множества id отличаются. Будем брать пересечение.')\n",
    "\n",
    "common_ids = sorted(list(ids1 & ids2))\n",
    "print('Будем работать с', len(common_ids), 'общими id')\n",
    "\n",
    "sub1_common = sub1[sub1[ID_COL].isin(common_ids)].copy()\n",
    "sub2_common = sub2[sub2[ID_COL].isin(common_ids)].copy()\n",
    "\n",
    "# сортируем по id для аккуратного merge\n",
    "sub1_common.sort_values(ID_COL, inplace=True)\n",
    "sub2_common.sort_values(ID_COL, inplace=True)\n",
    "\n",
    "blend_df = sub1_common[[ID_COL]].copy()\n",
    "blend_df['answer_1'] = sub1_common[ANSWER_COL].astype(str).fillna('')\n",
    "blend_df['answer_2'] = sub2_common[ANSWER_COL].astype(str).fillna('')\n",
    "\n",
    "if REFS_COL in sub1_common.columns and REFS_COL in sub2_common.columns:\n",
    "    blend_df['refs_1'] = sub1_common[REFS_COL].astype(str).fillna('')\n",
    "    blend_df['refs_2'] = sub2_common[REFS_COL].astype(str).fillna('')\n",
    "else:\n",
    "    blend_df['refs_1'] = ''\n",
    "    blend_df['refs_2'] = ''\n",
    "\n",
    "print('\\nblend_df preview:')\n",
    "blend_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0aeaa",
   "metadata": {},
   "source": [
    "## Block 2. Вспомогательные функции (валидность, длина, похожесть)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c81fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_invalid_answer(text: str) -> bool:\n",
    "    \"\"\"Эвристика: пустой текст, nan, явные ошибки.\"\"\"\n",
    "    if text is None:\n",
    "        return True\n",
    "    t = str(text).strip()\n",
    "    if not t:\n",
    "        return True\n",
    "    # часто при RAG встречается что-то вроде 'error: ...'\n",
    "    lowered = t.lower()\n",
    "    if lowered.startswith('error:') or 'traceback' in lowered:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def answer_length(text: str) -> int:\n",
    "    if text is None:\n",
    "        return 0\n",
    "    return len(str(text))\n",
    "\n",
    "def simple_similarity(a: str, b: str) -> float:\n",
    "    \"\"\"Приблизительная похожесть на [0,1] по SequenceMatcher.\"\"\"\n",
    "    a = (a or '').strip()\n",
    "    b = (b or '').strip()\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    if text is None:\n",
    "        return []\n",
    "    return re.findall(r\"\\w+\", str(text).lower(), flags=re.UNICODE)\n",
    "\n",
    "def jaccard_tokens(a: str, b: str) -> float:\n",
    "    ta = set(tokenize(a))\n",
    "    tb = set(tokenize(b))\n",
    "    if not ta and not tb:\n",
    "        return 1.0\n",
    "    if not ta or not tb:\n",
    "        return 0.0\n",
    "    inter = len(ta & tb)\n",
    "    union = len(ta | tb)\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return inter / union\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae8f6d",
   "metadata": {},
   "source": [
    "## Block 3. Правила blending: как выбирать лучший ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4feccd",
   "metadata": {},
   "source": [
    "Базовая идея: для каждого `id` у нас есть `answer_1` и `answer_2`.\n",
    "\n",
    "Мы хотим вернуть `final_answer` и желательно отметить, откуда он взят (`src = 1/2/mix`).\n",
    "\n",
    "Эвристики:\n",
    "\n",
    "1. Если один ответ явно **невалидный** (пустой / `error:`), а второй — нет → берём валидный.\n",
    "2. Если один ответ **очень короткий**, а другой длиннее `MIN_ANSWER_LEN` → берём длинный.\n",
    "3. Если один ответ **подозрительно длинный** (> `MAX_ANSWER_LEN`), а другой короче → берём короткий.\n",
    "4. Если ответы **очень похожи** (`similarity > VERY_SIMILAR_THRESHOLD`) → берём более краткий (или от первого сабмита).\n",
    "5. Иначе по умолчанию доверяем, например, `sub2` (считаем его более сильным) — *или наоборот*, это можно настроить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_two_answers(a1: str, a2: str) -> Dict[str, Any]:\n",
    "    a1 = str(a1 or '')\n",
    "    a2 = str(a2 or '')\n",
    "\n",
    "    invalid_1 = is_invalid_answer(a1)\n",
    "    invalid_2 = is_invalid_answer(a2)\n",
    "\n",
    "    len1 = answer_length(a1)\n",
    "    len2 = answer_length(a2)\n",
    "\n",
    "    sim_seq = simple_similarity(a1, a2)\n",
    "    sim_jac = jaccard_tokens(a1, a2)\n",
    "\n",
    "    # 1) если один ответ явно невалидный → берём другой\n",
    "    if invalid_1 and not invalid_2:\n",
    "        return {'answer': a2, 'src': 2, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "    if invalid_2 and not invalid_1:\n",
    "        return {'answer': a1, 'src': 1, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "\n",
    "    # 2) если оба невалидные → возвращаем первый как есть\n",
    "    if invalid_1 and invalid_2:\n",
    "        return {'answer': a1, 'src': 1, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "\n",
    "    # 3) короткие ответы vs длинные\n",
    "    if len1 < MIN_ANSWER_LEN <= len2:\n",
    "        return {'answer': a2, 'src': 2, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "    if len2 < MIN_ANSWER_LEN <= len1:\n",
    "        return {'answer': a1, 'src': 1, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "\n",
    "    # 4) слишком длинные ответы\n",
    "    if len1 > MAX_ANSWER_LEN and len2 <= MAX_ANSWER_LEN:\n",
    "        return {'answer': a2, 'src': 2, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "    if len2 > MAX_ANSWER_LEN and len1 <= MAX_ANSWER_LEN:\n",
    "        return {'answer': a1, 'src': 1, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "\n",
    "    # 5) ответы почти одинаковые → берём более короткий\n",
    "    if sim_seq >= VERY_SIMILAR_THRESHOLD or sim_jac >= VERY_SIMILAR_THRESHOLD:\n",
    "        if len1 <= len2:\n",
    "            return {'answer': a1, 'src': 1, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "        else:\n",
    "            return {'answer': a2, 'src': 2, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n",
    "\n",
    "    # 6) дефолт: доверяем sub2 как более \"сильному\" (это можно поменять)\n",
    "    return {'answer': a2, 'src': 2, 'sim_seq': sim_seq, 'sim_jac': sim_jac}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3bf2a4",
   "metadata": {},
   "source": [
    "## Block 4. Применяем blending ко всем id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40858621",
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_results = []\n",
    "\n",
    "for _, row in tqdm(blend_df.iterrows(), total=len(blend_df), desc='Blending'):\n",
    "    a1 = row['answer_1']\n",
    "    a2 = row['answer_2']\n",
    "    res = blend_two_answers(a1, a2)\n",
    "    blend_results.append(res)\n",
    "\n",
    "blend_meta = pd.DataFrame(blend_results)\n",
    "blend_meta.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce0c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = blend_df[[ID_COL]].copy()\n",
    "final_df[ANSWER_COL] = blend_meta['answer']\n",
    "final_df['src'] = blend_meta['src']\n",
    "final_df['sim_seq'] = blend_meta['sim_seq']\n",
    "final_df['sim_jac'] = blend_meta['sim_jac']\n",
    "\n",
    "# Если есть refs_json в обоих сабмитах — берём рефы из того сабмита, чей ответ выбран\n",
    "final_refs = []\n",
    "for i, row in final_df.iterrows():\n",
    "    src = row['src']\n",
    "    if src == 1:\n",
    "        final_refs.append(blend_df.loc[i, 'refs_1'])\n",
    "    elif src == 2:\n",
    "        final_refs.append(blend_df.loc[i, 'refs_2'])\n",
    "    else:\n",
    "        final_refs.append('')\n",
    "\n",
    "final_df[REFS_COL] = final_refs\n",
    "\n",
    "print('final_df shape:', final_df.shape)\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf859e",
   "metadata": {},
   "source": [
    "## Block 5. (Опционально) Оффлайн-оценка по ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73bd09",
   "metadata": {},
   "source": [
    "Если у тебя есть файл с правильными ответами `ground_truth.csv` такого формата:\n",
    "\n",
    "- `id` — вопрос id;\n",
    "- `answer` — правильный ответ (или эталонный текст),\n",
    "\n",
    "можно посчитать простую метрику (token-level F1) для:\n",
    "\n",
    "- сабмита 1;\n",
    "- сабмита 2;\n",
    "- blended-сабмита.\n",
    "\n",
    "Это удобно, чтобы быстро проверить, выгоден ли blending на локальных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_f1(pred: str, gt: str) -> float:\n",
    "    pred_tokens = tokenize(pred)\n",
    "    gt_tokens = tokenize(gt)\n",
    "    if not pred_tokens or not gt_tokens:\n",
    "        return 0.0\n",
    "    common = set(pred_tokens) & set(gt_tokens)\n",
    "    if not common:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gt_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "if GT_PATH.exists():\n",
    "    gt_df = pd.read_csv(GT_PATH)\n",
    "    if ID_COL not in gt_df.columns or ANSWER_COL not in gt_df.columns:\n",
    "        raise KeyError(f'В ground truth должны быть колонки {ID_COL!r} и {ANSWER_COL!r}')\n",
    "\n",
    "    # совмещаем с финальными сабмитами\n",
    "    eval_df = gt_df[[ID_COL, ANSWER_COL]].rename(columns={ANSWER_COL: 'gt_answer'})\n",
    "    eval_df = eval_df.merge(sub1[[ID_COL, ANSWER_COL]].rename(columns={ANSWER_COL: 'pred_1'}), on=ID_COL, how='left')\n",
    "    eval_df = eval_df.merge(sub2[[ID_COL, ANSWER_COL]].rename(columns={ANSWER_COL: 'pred_2'}), on=ID_COL, how='left')\n",
    "    eval_df = eval_df.merge(final_df[[ID_COL, ANSWER_COL]].rename(columns={ANSWER_COL: 'pred_blend'}), on=ID_COL, how='left')\n",
    "\n",
    "    f1_1 = []\n",
    "    f1_2 = []\n",
    "    f1_b = []\n",
    "    for _, r in eval_df.iterrows():\n",
    "        gt = str(r['gt_answer'])\n",
    "        f1_1.append(simple_f1(str(r['pred_1']), gt))\n",
    "        f1_2.append(simple_f1(str(r['pred_2']), gt))\n",
    "        f1_b.append(simple_f1(str(r['pred_blend']), gt))\n",
    "\n",
    "    print('\\n=== Оффлайн-оценка по token-F1 ===')\n",
    "    print('sub1   mean F1:', float(np.mean(f1_1)))\n",
    "    print('sub2   mean F1:', float(np.mean(f1_2)))\n",
    "    print('blend  mean F1:', float(np.mean(f1_b)))\n",
    "else:\n",
    "    print('GT_PATH не существует, оффлайн-оценка пропущена:', GT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc5011",
   "metadata": {},
   "source": [
    "## Block 6. Сохранение blended-сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e896c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEND_PATH = OUTPUT_DIR / 'submission_blended.csv'\n",
    "\n",
    "# Для сабмита обычно нужны только id и answer (и, возможно, refs_json)\n",
    "cols_for_submit = [ID_COL, ANSWER_COL]\n",
    "if REFS_COL in final_df.columns:\n",
    "    cols_for_submit.append(REFS_COL)\n",
    "\n",
    "submit_blend = final_df[cols_for_submit].copy()\n",
    "submit_blend.to_csv(BLEND_PATH, index=False)\n",
    "print('Blended submission saved to:', BLEND_PATH)\n",
    "submit_blend.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb12c85",
   "metadata": {},
   "source": [
    "## Итого\n",
    "\n",
    "В этом ноутбуке:\n",
    "\n",
    "1. Мы загрузили два сабмита `sub1` и `sub2` и выровняли их по `id`.\n",
    "2. Для каждого `id` посчитали длину ответов и два типа похожести (SequenceMatcher и Jaccard по токенам).\n",
    "3. С помощью простых правил выбрали финальный ответ `final_df[answer]` и источник (`src = 1/2`).\n",
    "4. (Опционально) Посмотрели, улучшает ли blending F1 на локальном ground truth.\n",
    "5. Сохранили итоговый файл `submission_blended.csv`.\n",
    "\n",
    "На финале ты можешь быстро подстроить пороги `MIN_ANSWER_LEN`, `MAX_ANSWER_LEN`,\n",
    "`VERY_SIMILAR_THRESHOLD` и стратегию по умолчанию (кому доверять больше — `sub1` или `sub2`)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}