{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d8eda8",
   "metadata": {},
   "source": [
    "# RAG Troubleshooting & Tips\n",
    "\n",
    "Шпаргалка по частым ошибкам, багам и полезным приёмам для RAG-ноутбуков.\n",
    "\n",
    "Можно положить в репозиторий как `RAG_Troubleshooting.ipynb` или `docs/RAG_troubleshooting.md`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac79fef",
   "metadata": {},
   "source": [
    "## 0. Структура репозитория (рекомендация)\n",
    "\n",
    "Пример структуры репы с RAG-ноутбуками:\n",
    "\n",
    "```text\n",
    "rag-project/\n",
    "  ├─ README.md\n",
    "  ├─ RAG_Baseline_AutoRAG.ipynb              # основной пайплайн (autorag_style_rag_pipeline)\n",
    "  ├─ RAG_Simple_Playground.ipynb             # простой playground (simple_rag_playground)\n",
    "  ├─ RAG_Experiments_Eval_Lab.ipynb          # лаборатория экспериментов (rag_experiments_eval_lab)\n",
    "  ├─ RAG_Troubleshooting.ipynb               # этот файл\n",
    "  ├─ configs/\n",
    "  │    ├─ rag_config_example.yaml            # по желанию — конфиги\n",
    "  └─ data/\n",
    "       ├─ docs.csv / docs/                   # локальные данные\n",
    "       ├─ qa.csv\n",
    "       └─ sample_submission.csv\n",
    "```\n",
    "\n",
    "Идея: один baseline, один playground, одна «лаборатория» и один файл со шпаргалкой.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33897f5",
   "metadata": {},
   "source": [
    "## 1. Быстрая проверка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers, datasets, platform, sys\n",
    "print('Python :', sys.version)\n",
    "print('OS     :', platform.platform())\n",
    "print('Torch  :', torch.__version__)\n",
    "print('GPU    :', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU only')\n",
    "print('HF     :', transformers.__version__)\n",
    "print('datasets:', datasets.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be7d04",
   "metadata": {},
   "source": [
    "Если что-то падает, сначала убедись, что:\n",
    "\n",
    "- `torch` видит GPU;\n",
    "- версии `transformers` и `sentence-transformers` совместимы (в кластерных средах это может быть жёстко зафиксировано);\n",
    "- ты не на Python 3.12 с очень старыми библиотеками (или наоборот).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9311019b",
   "metadata": {},
   "source": [
    "## 2. Проблемы с загрузкой моделей/токенайзеров HF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb648cd",
   "metadata": {},
   "source": [
    "Частые ошибки:\n",
    "\n",
    "- `OSError: Can't load tokenizer for '...'`;\n",
    "- `We couldn't connect to 'https://huggingface.co' ...` (нет интернета);\n",
    "- зависание при скачивании модели.\n",
    "\n",
    "### Решения\n",
    "\n",
    "1. **Используй локальный путь**, а не имя из HF Hub.\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_PATH = '/kaggle/input/qwen2.5/transformers/1.5b-instruct/1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "```\n",
    "\n",
    "2. Для `SentenceTransformer`:\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBED_MODEL_PATH = '/kaggle/input/intfloat-multilingual-e5-large'\n",
    "model = SentenceTransformer(EMBED_MODEL_PATH, device='cuda')  # без похода в интернет\n",
    "```\n",
    "\n",
    "3. Если вообще нет токенайзера:\n",
    "\n",
    "- проверь, что в папке есть `tokenizer.json` или `tokenizer_config.json`;\n",
    "- если модель quantized / кастомная, иногда нужно ставить правильную версию `transformers`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0c797",
   "metadata": {},
   "source": [
    "## 3. CUDA out of memory (OOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebc26e",
   "metadata": {},
   "source": [
    "Симптом: `RuntimeError: CUDA out of memory ...`\n",
    "\n",
    "### Как лечить\n",
    "\n",
    "1. **Уменьшить batch size** при эмбеддинге:\n",
    "\n",
    "```python\n",
    "vectors = embedding_model.encode(\n",
    "    texts,\n",
    "    batch_size=16,  # было 64\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "```\n",
    "\n",
    "2. **Уменьшить размер чанка / число чанков**:\n",
    "\n",
    "- `CHUNK_SIZE` поменьше,\n",
    "- `CHUNK_OVERLAP` поменьше\n",
    "(меньше текста → меньше RAM и VRAM).\n",
    "\n",
    "3. **Уменьшить TOP-K** в retrieval:\n",
    "\n",
    "- `TOP_K_RETRIEVAL`, `TOP_K_FINAL` сделали меньше;\n",
    "- меньше контекста — меньше токенов у LLM.\n",
    "\n",
    "4. **Использовать float16** для генерации (в ноутбуках уже стоит):\n",
    "\n",
    "```python\n",
    "generation_pipeline = pipeline(\n",
    "    'text-generation',\n",
    "    model=GEN_MODEL_PATH,\n",
    "    device=0,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "```\n",
    "\n",
    "5. **Чистить GPU-память между большими шагами** (если прям прижало):\n",
    "\n",
    "```python\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80fdc0",
   "metadata": {},
   "source": [
    "## 4. Expected all tensors to be on the same device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9893f02",
   "metadata": {},
   "source": [
    "Ошибка вида:\n",
    "\n",
    "`RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!`\n",
    "\n",
    "### Причины\n",
    "\n",
    "- модель на GPU, а входной тензор на CPU (или наоборот);\n",
    "- несколько моделей с разными `device`.\n",
    "\n",
    "### Что делать\n",
    "\n",
    "1. Убедись, что везде используешь один и тот же `device`:\n",
    "\n",
    "```python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "embedding_model = SentenceTransformer(EMBED_MODEL_ID, device=str(device))\n",
    "generation_pipeline = pipeline(\n",
    "    'text-generation',\n",
    "    model=GEN_MODEL_PATH,\n",
    "    device=0 if device.type == 'cuda' else -1,\n",
    ")\n",
    "```\n",
    "\n",
    "2. Если где-то вручную создаёшь тензоры, явно переносить на `device`:\n",
    "\n",
    "```python\n",
    "q_vec = torch.tensor(q_vec).to(device)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db675d",
   "metadata": {},
   "source": [
    "## 5. Ошибки с `pad_token_id` / `eos_token_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94318d1c",
   "metadata": {},
   "source": [
    "Часто у LLM (особенно Qwen/LLaMA-подобных) нет `pad_token_id` по умолчанию.\n",
    "\n",
    "Ошибка:\n",
    "\n",
    "- `ValueError: You have to specify either pad_token_id or eos_token_id`;\n",
    "- или предупреждения от `transformers`.\n",
    "\n",
    "### Быстрый фикс\n",
    "\n",
    "Если используешь `pipeline`, можно забрать `eos_token_id` из токенайзера и пробросить как `pad_token_id`:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "MODEL_PATH = GEN_MODEL_PATH\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "generation_pipeline = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "```\n",
    "\n",
    "В наших ноутбуках обычно хватает передачи `pad_token_id=generation_pipeline.tokenizer.eos_token_id` в `pipeline(...).`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d2282",
   "metadata": {},
   "source": [
    "## 6. `datasets` + отсутствие интернета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ed441",
   "metadata": {},
   "source": [
    "Если среда без интернета (кластер, финал соревнования), то `load_dataset(...)` может упасть при первой загрузке.\n",
    "\n",
    "### Стратегия\n",
    "\n",
    "1. **Заранее выгрузи датасет в CSV/Parquet** и клади в `/kaggle/input` / локальную папку.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('your/dataset', split='train')\n",
    "df = ds.to_pandas()\n",
    "df.to_csv('docs.csv', index=False)\n",
    "```\n",
    "\n",
    "2. В боевых ноутбуках вместо `load_dataset` используй `pd.read_csv`.\n",
    "\n",
    "3. Если очень надо `datasets`, можно передать `cache_dir` и надеяться, что кэш уже есть в среде:\n",
    "\n",
    "```python\n",
    "ds = load_dataset('your/dataset', split='train', cache_dir='/kaggle/input/hf-cache')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f2309",
   "metadata": {},
   "source": [
    "## 7. Ошибки с колонками (`KeyError`, `ValueError`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6576cc0",
   "metadata": {},
   "source": [
    "Типичные кейсы:\n",
    "\n",
    "- `KeyError: 'question'` — в QA-файле колонка называется иначе (`text`, `query` и т.д.);\n",
    "- `ValueError: Колонка ... не найдена` — в наших helper-функциях жёстко зашито имя.\n",
    "\n",
    "### Что делать\n",
    "\n",
    "1. Сразу после чтения данных **печатай первые строки** и список колонок:\n",
    "\n",
    "```python\n",
    "print(qa_df.columns)\n",
    "qa_df.head()\n",
    "```\n",
    "\n",
    "2. Приведи всё к стандартным именам в одном месте:\n",
    "\n",
    "```python\n",
    "qa_df = qa_df.rename(columns={\n",
    "    'query_text': 'question',\n",
    "    'id_col': 'id',\n",
    "})\n",
    "```\n",
    "\n",
    "3. В конфиге ноутбука собери все важные имена колонок:\n",
    "\n",
    "```python\n",
    "QA_ID_COL = 'id'\n",
    "QA_QUESTION_COL = 'question'\n",
    "QA_GT_ANSWER_COL = 'answer'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf12ef",
   "metadata": {},
   "source": [
    "## 8. Как дебажить качество ретрива (до LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378f964",
   "metadata": {},
   "source": [
    "Чтобы понять, что RAG тупит из‑за плохого retriever, полезно смотреть, **какие чанки достаются**, без LLM.\n",
    "\n",
    "Пример helper-функции (можно переиспользовать в любых ноутбуках):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f22869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_retrieval(query: str, mode: str = 'hybrid', top_k: int = 5):\n",
    "    if mode == 'dense':\n",
    "        retrieved = dense_search(query, top_k=top_k)\n",
    "    elif mode == 'bm25':\n",
    "        retrieved = bm25_search(query, top_k=top_k)\n",
    "    else:\n",
    "        # для hybrid_search в некоторых ноутах другая сигнатура — подстрой\n",
    "        retrieved = hybrid_search(query, top_k_final=top_k, top_k_dense=top_k, top_k_bm25=top_k, alpha=0.5)\n",
    "\n",
    "    print('QUESTION:')\n",
    "    print(query)\n",
    "    print('\\nRETRIEVED CHUNKS:')\n",
    "    for i, row in retrieved.iterrows():\n",
    "        print('=' * 80)\n",
    "        print(f\"rank={i} | doc_id={row['doc_id']} | chunk_id={row['chunk_id']}\")\n",
    "        for col in ['score_dense', 'score_bm25', 'score_hybrid']:\n",
    "            if col in row and not pd.isna(row[col]):\n",
    "                print(f\"{col} = {row[col]:.4f}\")\n",
    "        print('-' * 40)\n",
    "        print(row['text'][:800])\n",
    "        print('\\n')\n",
    "\n",
    "# пример:\n",
    "# inspect_retrieval('What is the main idea of the document?', mode='hybrid', top_k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9f0ff",
   "metadata": {},
   "source": [
    "## 9. Повторяемость (seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3140c",
   "metadata": {},
   "source": [
    "Чтобы результаты экспериментов были воспроизводимыми:\n",
    "\n",
    "```python\n",
    "import random, numpy as np, torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "```\n",
    "\n",
    "В наших ноутбуках это уже есть, но при копипасте отдельных кусков кода не забудь перенести блок с SEED.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ebbc7",
   "metadata": {},
   "source": [
    "## 10. Небольшие performance‑советы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8232bbb7",
   "metadata": {},
   "source": [
    "- Эмбеддинги:\n",
    "  - Один раз посчитай эмбеддинги для всех чанков и **сохрани в `.npy`/`.parquet`**, чтобы не пересчитывать при каждом эксперименте.\n",
    "\n",
    "  ```python\n",
    "  np.save('dense_matrix.npy', dense_matrix)\n",
    "  # а потом\n",
    "  dense_matrix = np.load('dense_matrix.npy')\n",
    "  ```\n",
    "\n",
    "- Retrieval:\n",
    "  - експериментируй с `TOP_K_RETRIEVAL` / `TOP_K_FINAL`: часто 8–10 чанков достаточно;\n",
    "  - не тащи 50 чанков в LLM, если лимит токенов маленький.\n",
    "\n",
    "- LLM:\n",
    "  - держи промпт максимально коротким;\n",
    "  - выключи `do_sample=True` для более стабильных результатов метрик (для отбора лучшей конфигурации), а потом включай stochastics для финального сабмита.\n",
    "\n",
    "```python\n",
    "out = generation_pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    do_sample=False,  # для оффлайн-оценки\n",
    ")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}